
2018-05-24 00:11:25 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: jd_spider)
2018-05-24 00:11:25 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (v3.5.2:4def2a2901a5, Jun 25 2016, 22:18:55) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 17.5.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-10-10.0.16299-SP0
2018-05-24 00:11:25 [scrapy.crawler] INFO: Overridden settings: {'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'DOWNLOAD_TIMEOUT': 30, 'LOG_FILE': 'JdSpider.log', 'BOT_NAME': 'jd_spider', 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'SPIDER_MODULES': ['jd_spider.spiders'], 'LOG_LEVEL': 'INFO', 'SPIDER_LOADER_WARN_ONLY': True, 'NEWSPIDER_MODULE': 'jd_spider.spiders'}
2018-05-24 00:11:26 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole']
2018-05-24 00:11:26 [jd] INFO: Reading start URLs from redis key 'JdSpider:start_urls' (batch size: 16, encoding: utf-8
2018-05-24 00:11:26 [scrapy.middleware] INFO: Enabled downloader middlewares:
['jd_spider.middlewares.JdSpiderDownloaderMiddleware',
 'jd_spider.middlewares.RandomProxy',
 'jd_spider.middlewares.MyRetryMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-05-24 00:11:26 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-05-24 00:11:26 [scrapy.middleware] INFO: Enabled item pipelines:
['scrapy_redis.pipelines.RedisPipeline']
2018-05-24 00:11:26 [scrapy.core.engine] INFO: Spider opened
2018-05-24 00:11:26 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-05-24 00:11:26 [jd] INFO: Spider opened: jd
2018-05-24 00:12:26 [scrapy.extensions.logstats] INFO: Crawled 354 pages (at 354 pages/min), scraped 2729 items (at 2729 items/min)
2018-05-24 00:12:54 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_28185026128&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527079669574>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 00:13:26 [scrapy.extensions.logstats] INFO: Crawled 1400 pages (at 1046 pages/min), scraped 3561 items (at 832 items/min)
2018-05-24 00:14:26 [scrapy.extensions.logstats] INFO: Crawled 6566 pages (at 5166 pages/min), scraped 10411 items (at 6850 items/min)
2018-05-24 00:15:26 [scrapy.extensions.logstats] INFO: Crawled 12208 pages (at 5642 pages/min), scraped 18585 items (at 8174 items/min)
2018-05-24 00:16:26 [scrapy.extensions.logstats] INFO: Crawled 18902 pages (at 6694 pages/min), scraped 29780 items (at 11195 items/min)
2018-05-24 00:17:26 [scrapy.extensions.logstats] INFO: Crawled 23919 pages (at 5017 pages/min), scraped 39060 items (at 9280 items/min)
2018-05-24 00:18:26 [scrapy.extensions.logstats] INFO: Crawled 31087 pages (at 7168 pages/min), scraped 45591 items (at 6531 items/min)
2018-05-24 00:19:26 [scrapy.extensions.logstats] INFO: Crawled 36215 pages (at 5128 pages/min), scraped 54105 items (at 8514 items/min)
2018-05-24 00:20:26 [scrapy.extensions.logstats] INFO: Crawled 42772 pages (at 6557 pages/min), scraped 57332 items (at 3227 items/min)
2018-05-24 00:21:26 [scrapy.extensions.logstats] INFO: Crawled 48839 pages (at 6067 pages/min), scraped 64391 items (at 7059 items/min)
2018-05-24 00:22:26 [scrapy.extensions.logstats] INFO: Crawled 54211 pages (at 5372 pages/min), scraped 65947 items (at 1556 items/min)
2018-05-24 00:23:26 [scrapy.extensions.logstats] INFO: Crawled 59569 pages (at 5358 pages/min), scraped 71755 items (at 5808 items/min)
2018-05-24 00:24:26 [scrapy.extensions.logstats] INFO: Crawled 65037 pages (at 5468 pages/min), scraped 72189 items (at 434 items/min)
2018-05-24 00:25:26 [scrapy.extensions.logstats] INFO: Crawled 70257 pages (at 5220 pages/min), scraped 74934 items (at 2745 items/min)
2018-05-24 00:26:26 [scrapy.extensions.logstats] INFO: Crawled 75280 pages (at 5023 pages/min), scraped 78330 items (at 3396 items/min)
2018-05-24 00:27:26 [scrapy.extensions.logstats] INFO: Crawled 80751 pages (at 5471 pages/min), scraped 78760 items (at 430 items/min)
2018-05-24 00:28:26 [scrapy.extensions.logstats] INFO: Crawled 86167 pages (at 5416 pages/min), scraped 79494 items (at 734 items/min)
2018-05-24 00:29:26 [scrapy.extensions.logstats] INFO: Crawled 91411 pages (at 5244 pages/min), scraped 81328 items (at 1834 items/min)
2018-05-24 00:30:26 [scrapy.extensions.logstats] INFO: Crawled 96701 pages (at 5290 pages/min), scraped 83551 items (at 2223 items/min)
2018-05-24 00:31:26 [scrapy.extensions.logstats] INFO: Crawled 102057 pages (at 5356 pages/min), scraped 90306 items (at 6755 items/min)
2018-05-24 00:32:26 [scrapy.extensions.logstats] INFO: Crawled 108580 pages (at 6523 pages/min), scraped 97709 items (at 7403 items/min)
2018-05-24 00:33:26 [scrapy.extensions.logstats] INFO: Crawled 113675 pages (at 5095 pages/min), scraped 104109 items (at 6400 items/min)
2018-05-24 00:34:26 [scrapy.extensions.logstats] INFO: Crawled 119269 pages (at 5594 pages/min), scraped 109159 items (at 5050 items/min)
2018-05-24 00:35:26 [scrapy.extensions.logstats] INFO: Crawled 124587 pages (at 5318 pages/min), scraped 109831 items (at 672 items/min)
2018-05-24 00:36:26 [scrapy.extensions.logstats] INFO: Crawled 130277 pages (at 5690 pages/min), scraped 109831 items (at 0 items/min)
2018-05-24 00:37:26 [scrapy.extensions.logstats] INFO: Crawled 135644 pages (at 5367 pages/min), scraped 113095 items (at 3264 items/min)
2018-05-24 00:38:26 [scrapy.extensions.logstats] INFO: Crawled 140320 pages (at 4676 pages/min), scraped 125871 items (at 12776 items/min)
2018-05-24 00:39:26 [scrapy.extensions.logstats] INFO: Crawled 145961 pages (at 5641 pages/min), scraped 125871 items (at 0 items/min)
2018-05-24 00:40:26 [scrapy.extensions.logstats] INFO: Crawled 150368 pages (at 4407 pages/min), scraped 135749 items (at 9878 items/min)
2018-05-24 00:41:26 [scrapy.extensions.logstats] INFO: Crawled 155533 pages (at 5165 pages/min), scraped 142840 items (at 7091 items/min)
2018-05-24 00:42:26 [scrapy.extensions.logstats] INFO: Crawled 160627 pages (at 5094 pages/min), scraped 150839 items (at 7999 items/min)
2018-05-24 00:43:26 [scrapy.extensions.logstats] INFO: Crawled 165598 pages (at 4971 pages/min), scraped 157039 items (at 6200 items/min)
2018-05-24 00:44:26 [scrapy.extensions.logstats] INFO: Crawled 170288 pages (at 4690 pages/min), scraped 157782 items (at 743 items/min)
2018-05-24 00:45:26 [scrapy.extensions.logstats] INFO: Crawled 175535 pages (at 5247 pages/min), scraped 163683 items (at 5901 items/min)
2018-05-24 00:46:26 [scrapy.extensions.logstats] INFO: Crawled 180740 pages (at 5205 pages/min), scraped 169459 items (at 5776 items/min)
2018-05-24 00:47:26 [scrapy.extensions.logstats] INFO: Crawled 185257 pages (at 4517 pages/min), scraped 171702 items (at 2243 items/min)
2018-05-24 00:48:26 [scrapy.extensions.logstats] INFO: Crawled 190175 pages (at 4918 pages/min), scraped 172355 items (at 653 items/min)
2018-05-24 00:49:26 [scrapy.extensions.logstats] INFO: Crawled 196529 pages (at 6354 pages/min), scraped 173193 items (at 838 items/min)
2018-05-24 00:50:26 [scrapy.extensions.logstats] INFO: Crawled 202450 pages (at 5921 pages/min), scraped 174099 items (at 906 items/min)
2018-05-24 00:51:26 [scrapy.extensions.logstats] INFO: Crawled 202910 pages (at 460 pages/min), scraped 174160 items (at 61 items/min)
2018-05-24 00:52:12 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_19022196572&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527094125876>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2018-05-24 00:52:26 [scrapy.extensions.logstats] INFO: Crawled 203758 pages (at 848 pages/min), scraped 174263 items (at 103 items/min)
2018-05-24 00:53:26 [scrapy.extensions.logstats] INFO: Crawled 205614 pages (at 1856 pages/min), scraped 174460 items (at 197 items/min)
2018-05-24 00:54:26 [scrapy.extensions.logstats] INFO: Crawled 210179 pages (at 4565 pages/min), scraped 174927 items (at 467 items/min)
2018-05-24 00:55:26 [scrapy.extensions.logstats] INFO: Crawled 216794 pages (at 6615 pages/min), scraped 175614 items (at 687 items/min)
2018-05-24 00:56:26 [scrapy.extensions.logstats] INFO: Crawled 222835 pages (at 6041 pages/min), scraped 176245 items (at 631 items/min)
2018-05-24 00:57:26 [scrapy.extensions.logstats] INFO: Crawled 229716 pages (at 6881 pages/min), scraped 176994 items (at 749 items/min)
2018-05-24 00:58:26 [scrapy.extensions.logstats] INFO: Crawled 236667 pages (at 6951 pages/min), scraped 177692 items (at 698 items/min)
2018-05-24 00:59:26 [scrapy.extensions.logstats] INFO: Crawled 243543 pages (at 6876 pages/min), scraped 178581 items (at 889 items/min)
2018-05-24 01:00:26 [scrapy.extensions.logstats] INFO: Crawled 250614 pages (at 7071 pages/min), scraped 179272 items (at 691 items/min)
2018-05-24 01:01:26 [scrapy.extensions.logstats] INFO: Crawled 257401 pages (at 6787 pages/min), scraped 179992 items (at 720 items/min)
2018-05-24 01:02:26 [scrapy.extensions.logstats] INFO: Crawled 264214 pages (at 6813 pages/min), scraped 180675 items (at 683 items/min)
2018-05-24 01:03:26 [scrapy.extensions.logstats] INFO: Crawled 270964 pages (at 6750 pages/min), scraped 181348 items (at 673 items/min)
2018-05-24 01:04:26 [scrapy.extensions.logstats] INFO: Crawled 277905 pages (at 6941 pages/min), scraped 182118 items (at 770 items/min)
2018-05-24 01:05:26 [scrapy.extensions.logstats] INFO: Crawled 284542 pages (at 6637 pages/min), scraped 183085 items (at 967 items/min)
2018-05-24 01:06:26 [scrapy.extensions.logstats] INFO: Crawled 291558 pages (at 7016 pages/min), scraped 183860 items (at 775 items/min)
2018-05-24 01:07:26 [scrapy.extensions.logstats] INFO: Crawled 298189 pages (at 6631 pages/min), scraped 184994 items (at 1134 items/min)
2018-05-24 01:08:26 [scrapy.extensions.logstats] INFO: Crawled 303955 pages (at 5766 pages/min), scraped 185569 items (at 575 items/min)
2018-05-24 01:09:26 [scrapy.extensions.logstats] INFO: Crawled 309565 pages (at 5610 pages/min), scraped 186129 items (at 560 items/min)
2018-05-24 01:10:26 [scrapy.extensions.logstats] INFO: Crawled 316307 pages (at 6742 pages/min), scraped 186774 items (at 645 items/min)
2018-05-24 01:11:26 [scrapy.extensions.logstats] INFO: Crawled 323117 pages (at 6810 pages/min), scraped 187398 items (at 624 items/min)
2018-05-24 01:12:26 [scrapy.extensions.logstats] INFO: Crawled 330030 pages (at 6913 pages/min), scraped 188054 items (at 656 items/min)
2018-05-24 01:13:26 [scrapy.extensions.logstats] INFO: Crawled 336581 pages (at 6551 pages/min), scraped 188689 items (at 635 items/min)
2018-05-24 01:14:26 [scrapy.extensions.logstats] INFO: Crawled 343726 pages (at 7145 pages/min), scraped 189318 items (at 629 items/min)
2018-05-24 01:15:26 [scrapy.extensions.logstats] INFO: Crawled 350772 pages (at 7046 pages/min), scraped 189964 items (at 646 items/min)
2018-05-24 01:16:26 [scrapy.extensions.logstats] INFO: Crawled 357605 pages (at 6833 pages/min), scraped 190543 items (at 579 items/min)
2018-05-24 01:17:26 [scrapy.extensions.logstats] INFO: Crawled 364693 pages (at 7088 pages/min), scraped 191176 items (at 633 items/min)
2018-05-24 01:18:26 [scrapy.extensions.logstats] INFO: Crawled 371687 pages (at 6994 pages/min), scraped 191802 items (at 626 items/min)
2018-05-24 01:19:26 [scrapy.extensions.logstats] INFO: Crawled 378332 pages (at 6645 pages/min), scraped 194268 items (at 2466 items/min)
2018-05-24 01:20:26 [scrapy.extensions.logstats] INFO: Crawled 384490 pages (at 6158 pages/min), scraped 196003 items (at 1735 items/min)
2018-05-24 01:21:26 [scrapy.extensions.logstats] INFO: Crawled 391572 pages (at 7082 pages/min), scraped 196837 items (at 834 items/min)
2018-05-24 01:22:26 [scrapy.extensions.logstats] INFO: Crawled 398522 pages (at 6950 pages/min), scraped 198032 items (at 1195 items/min)
2018-05-24 01:23:26 [scrapy.extensions.logstats] INFO: Crawled 405638 pages (at 7116 pages/min), scraped 198942 items (at 910 items/min)
2018-05-24 01:24:26 [scrapy.extensions.logstats] INFO: Crawled 412380 pages (at 6742 pages/min), scraped 200260 items (at 1318 items/min)
2018-05-24 01:25:26 [scrapy.extensions.logstats] INFO: Crawled 416268 pages (at 3888 pages/min), scraped 201130 items (at 870 items/min)
2018-05-24 01:26:26 [scrapy.extensions.logstats] INFO: Crawled 421440 pages (at 5172 pages/min), scraped 206115 items (at 4985 items/min)
2018-05-24 01:27:26 [scrapy.extensions.logstats] INFO: Crawled 428721 pages (at 7281 pages/min), scraped 208486 items (at 2371 items/min)
2018-05-24 01:28:26 [scrapy.extensions.logstats] INFO: Crawled 436065 pages (at 7344 pages/min), scraped 210385 items (at 1899 items/min)
2018-05-24 01:29:26 [scrapy.extensions.logstats] INFO: Crawled 442842 pages (at 6777 pages/min), scraped 212891 items (at 2506 items/min)
2018-05-24 01:30:26 [scrapy.extensions.logstats] INFO: Crawled 446226 pages (at 3384 pages/min), scraped 215048 items (at 2157 items/min)
2018-05-24 01:31:26 [scrapy.extensions.logstats] INFO: Crawled 453158 pages (at 6932 pages/min), scraped 220684 items (at 5636 items/min)
2018-05-24 01:32:26 [scrapy.extensions.logstats] INFO: Crawled 459937 pages (at 6779 pages/min), scraped 222157 items (at 1473 items/min)
2018-05-24 01:33:26 [scrapy.extensions.logstats] INFO: Crawled 467075 pages (at 7138 pages/min), scraped 224397 items (at 2240 items/min)
2018-05-24 01:34:26 [scrapy.extensions.logstats] INFO: Crawled 474216 pages (at 7141 pages/min), scraped 229658 items (at 5261 items/min)
2018-05-24 01:35:26 [scrapy.extensions.logstats] INFO: Crawled 481213 pages (at 6997 pages/min), scraped 238995 items (at 9337 items/min)
2018-05-24 01:36:26 [scrapy.extensions.logstats] INFO: Crawled 487535 pages (at 6322 pages/min), scraped 247263 items (at 8268 items/min)
2018-05-24 01:37:26 [scrapy.extensions.logstats] INFO: Crawled 493851 pages (at 6316 pages/min), scraped 248093 items (at 830 items/min)
2018-05-24 01:38:26 [scrapy.extensions.logstats] INFO: Crawled 499881 pages (at 6030 pages/min), scraped 251091 items (at 2998 items/min)
2018-05-24 01:39:26 [scrapy.extensions.logstats] INFO: Crawled 505424 pages (at 5543 pages/min), scraped 257175 items (at 6084 items/min)
2018-05-24 01:40:26 [scrapy.extensions.logstats] INFO: Crawled 512690 pages (at 7266 pages/min), scraped 267278 items (at 10103 items/min)
2018-05-24 01:41:26 [scrapy.extensions.logstats] INFO: Crawled 520208 pages (at 7518 pages/min), scraped 276897 items (at 9619 items/min)
2018-05-24 01:42:26 [scrapy.extensions.logstats] INFO: Crawled 527024 pages (at 6816 pages/min), scraped 289535 items (at 12638 items/min)
2018-05-24 01:43:26 [scrapy.extensions.logstats] INFO: Crawled 532831 pages (at 5807 pages/min), scraped 302797 items (at 13262 items/min)
2018-05-24 01:44:26 [scrapy.extensions.logstats] INFO: Crawled 539523 pages (at 6692 pages/min), scraped 318869 items (at 16072 items/min)
2018-05-24 01:45:26 [scrapy.extensions.logstats] INFO: Crawled 545884 pages (at 6361 pages/min), scraped 335011 items (at 16142 items/min)
2018-05-24 01:46:26 [scrapy.extensions.logstats] INFO: Crawled 552276 pages (at 6392 pages/min), scraped 339524 items (at 4513 items/min)
2018-05-24 01:47:26 [scrapy.extensions.logstats] INFO: Crawled 558619 pages (at 6343 pages/min), scraped 343205 items (at 3681 items/min)
2018-05-24 01:48:26 [scrapy.extensions.logstats] INFO: Crawled 564901 pages (at 6282 pages/min), scraped 344493 items (at 1288 items/min)
2018-05-24 01:49:26 [scrapy.extensions.logstats] INFO: Crawled 569831 pages (at 4930 pages/min), scraped 346739 items (at 2246 items/min)
2018-05-24 01:50:26 [scrapy.extensions.logstats] INFO: Crawled 576034 pages (at 6203 pages/min), scraped 352889 items (at 6150 items/min)
2018-05-24 01:51:26 [scrapy.extensions.logstats] INFO: Crawled 582105 pages (at 6071 pages/min), scraped 359256 items (at 6367 items/min)
2018-05-24 01:52:26 [scrapy.extensions.logstats] INFO: Crawled 587829 pages (at 5724 pages/min), scraped 366197 items (at 6941 items/min)
2018-05-24 01:53:26 [scrapy.extensions.logstats] INFO: Crawled 592754 pages (at 4925 pages/min), scraped 367953 items (at 1756 items/min)
2018-05-24 01:54:26 [scrapy.extensions.logstats] INFO: Crawled 597933 pages (at 5179 pages/min), scraped 368734 items (at 781 items/min)
2018-05-24 01:55:26 [scrapy.extensions.logstats] INFO: Crawled 603637 pages (at 5704 pages/min), scraped 370377 items (at 1643 items/min)
2018-05-24 01:56:26 [scrapy.extensions.logstats] INFO: Crawled 609705 pages (at 6068 pages/min), scraped 370939 items (at 562 items/min)
2018-05-24 01:57:26 [scrapy.extensions.logstats] INFO: Crawled 614851 pages (at 5146 pages/min), scraped 371378 items (at 439 items/min)
2018-05-24 01:58:26 [scrapy.extensions.logstats] INFO: Crawled 620722 pages (at 5871 pages/min), scraped 371925 items (at 547 items/min)
2018-05-24 01:59:26 [scrapy.extensions.logstats] INFO: Crawled 626633 pages (at 5911 pages/min), scraped 372505 items (at 580 items/min)
2018-05-24 02:00:26 [scrapy.extensions.logstats] INFO: Crawled 633129 pages (at 6496 pages/min), scraped 377597 items (at 5092 items/min)
2018-05-24 02:01:26 [scrapy.extensions.logstats] INFO: Crawled 639428 pages (at 6299 pages/min), scraped 379954 items (at 2357 items/min)
2018-05-24 02:02:26 [scrapy.extensions.logstats] INFO: Crawled 646347 pages (at 6919 pages/min), scraped 398028 items (at 18074 items/min)
2018-05-24 02:03:26 [scrapy.extensions.logstats] INFO: Crawled 653297 pages (at 6950 pages/min), scraped 413346 items (at 15318 items/min)
2018-05-24 02:04:26 [scrapy.extensions.logstats] INFO: Crawled 660269 pages (at 6972 pages/min), scraped 423901 items (at 10555 items/min)
2018-05-24 02:05:26 [scrapy.extensions.logstats] INFO: Crawled 667011 pages (at 6742 pages/min), scraped 439673 items (at 15772 items/min)
2018-05-24 02:06:26 [scrapy.extensions.logstats] INFO: Crawled 673744 pages (at 6733 pages/min), scraped 456698 items (at 17025 items/min)
2018-05-24 02:07:26 [scrapy.extensions.logstats] INFO: Crawled 679454 pages (at 5710 pages/min), scraped 457592 items (at 894 items/min)
2018-05-24 02:08:26 [scrapy.extensions.logstats] INFO: Crawled 685732 pages (at 6278 pages/min), scraped 458165 items (at 573 items/min)
2018-05-24 02:09:26 [scrapy.extensions.logstats] INFO: Crawled 692059 pages (at 6327 pages/min), scraped 458817 items (at 652 items/min)
2018-05-24 02:10:26 [scrapy.extensions.logstats] INFO: Crawled 697644 pages (at 5585 pages/min), scraped 459336 items (at 519 items/min)
2018-05-24 02:11:26 [scrapy.extensions.logstats] INFO: Crawled 703761 pages (at 6117 pages/min), scraped 459866 items (at 530 items/min)
2018-05-24 02:12:26 [scrapy.extensions.logstats] INFO: Crawled 709995 pages (at 6234 pages/min), scraped 460480 items (at 614 items/min)
2018-05-24 02:13:26 [scrapy.extensions.logstats] INFO: Crawled 716121 pages (at 6126 pages/min), scraped 461010 items (at 530 items/min)
2018-05-24 02:14:26 [scrapy.extensions.logstats] INFO: Crawled 722466 pages (at 6345 pages/min), scraped 461573 items (at 563 items/min)
2018-05-24 02:15:26 [scrapy.extensions.logstats] INFO: Crawled 728680 pages (at 6214 pages/min), scraped 462194 items (at 621 items/min)
2018-05-24 02:16:26 [scrapy.extensions.logstats] INFO: Crawled 735006 pages (at 6326 pages/min), scraped 463497 items (at 1303 items/min)
2018-05-24 02:17:26 [scrapy.extensions.logstats] INFO: Crawled 741288 pages (at 6282 pages/min), scraped 464214 items (at 717 items/min)
2018-05-24 02:18:26 [scrapy.extensions.logstats] INFO: Crawled 747563 pages (at 6275 pages/min), scraped 465289 items (at 1075 items/min)
2018-05-24 02:19:26 [scrapy.extensions.logstats] INFO: Crawled 753741 pages (at 6178 pages/min), scraped 466279 items (at 990 items/min)
2018-05-24 02:20:26 [scrapy.extensions.logstats] INFO: Crawled 759879 pages (at 6138 pages/min), scraped 467991 items (at 1712 items/min)
2018-05-24 02:21:26 [scrapy.extensions.logstats] INFO: Crawled 765163 pages (at 5284 pages/min), scraped 468936 items (at 945 items/min)
2018-05-24 02:22:26 [scrapy.extensions.logstats] INFO: Crawled 771288 pages (at 6125 pages/min), scraped 471769 items (at 2833 items/min)
2018-05-24 02:23:26 [scrapy.extensions.logstats] INFO: Crawled 776219 pages (at 4931 pages/min), scraped 487922 items (at 16153 items/min)
2018-05-24 02:24:26 [scrapy.extensions.logstats] INFO: Crawled 781993 pages (at 5774 pages/min), scraped 494171 items (at 6249 items/min)
2018-05-24 02:25:26 [scrapy.extensions.logstats] INFO: Crawled 787243 pages (at 5250 pages/min), scraped 496746 items (at 2575 items/min)
2018-05-24 02:26:26 [scrapy.extensions.logstats] INFO: Crawled 790948 pages (at 3705 pages/min), scraped 501074 items (at 4328 items/min)
2018-05-24 02:27:26 [scrapy.extensions.logstats] INFO: Crawled 794233 pages (at 3285 pages/min), scraped 505274 items (at 4200 items/min)
2018-05-24 02:28:26 [scrapy.extensions.logstats] INFO: Crawled 800121 pages (at 5888 pages/min), scraped 508112 items (at 2838 items/min)
2018-05-24 02:29:26 [scrapy.extensions.logstats] INFO: Crawled 803947 pages (at 3826 pages/min), scraped 511866 items (at 3754 items/min)
2018-05-24 02:30:26 [scrapy.extensions.logstats] INFO: Crawled 806242 pages (at 2295 pages/min), scraped 515856 items (at 3990 items/min)
2018-05-24 02:31:26 [scrapy.extensions.logstats] INFO: Crawled 810333 pages (at 4091 pages/min), scraped 518902 items (at 3046 items/min)
2018-05-24 02:32:26 [scrapy.extensions.logstats] INFO: Crawled 811590 pages (at 1257 pages/min), scraped 520190 items (at 1288 items/min)
2018-05-24 02:33:26 [scrapy.extensions.logstats] INFO: Crawled 816437 pages (at 4847 pages/min), scraped 522211 items (at 2021 items/min)
2018-05-24 02:34:26 [scrapy.extensions.logstats] INFO: Crawled 822681 pages (at 6244 pages/min), scraped 523917 items (at 1706 items/min)
2018-05-24 02:35:26 [scrapy.extensions.logstats] INFO: Crawled 828769 pages (at 6088 pages/min), scraped 527822 items (at 3905 items/min)
2018-05-24 02:36:26 [scrapy.extensions.logstats] INFO: Crawled 835268 pages (at 6499 pages/min), scraped 531805 items (at 3983 items/min)
2018-05-24 02:37:26 [scrapy.extensions.logstats] INFO: Crawled 842692 pages (at 7424 pages/min), scraped 539708 items (at 7903 items/min)
2018-05-24 02:38:26 [scrapy.extensions.logstats] INFO: Crawled 849380 pages (at 6688 pages/min), scraped 543966 items (at 4258 items/min)
2018-05-24 02:39:26 [scrapy.extensions.logstats] INFO: Crawled 856683 pages (at 7303 pages/min), scraped 548700 items (at 4734 items/min)
2018-05-24 02:40:26 [scrapy.extensions.logstats] INFO: Crawled 863745 pages (at 7062 pages/min), scraped 557323 items (at 8623 items/min)
2018-05-24 02:41:26 [scrapy.extensions.logstats] INFO: Crawled 870948 pages (at 7203 pages/min), scraped 563064 items (at 5741 items/min)
2018-05-24 02:42:26 [scrapy.extensions.logstats] INFO: Crawled 878241 pages (at 7293 pages/min), scraped 568555 items (at 5491 items/min)
2018-05-24 02:43:26 [scrapy.extensions.logstats] INFO: Crawled 882926 pages (at 4685 pages/min), scraped 573706 items (at 5151 items/min)
2018-05-24 02:44:26 [scrapy.extensions.logstats] INFO: Crawled 890141 pages (at 7215 pages/min), scraped 577856 items (at 4150 items/min)
2018-05-24 02:45:26 [scrapy.extensions.logstats] INFO: Crawled 895456 pages (at 5315 pages/min), scraped 596990 items (at 19134 items/min)
2018-05-24 02:46:26 [scrapy.extensions.logstats] INFO: Crawled 902747 pages (at 7291 pages/min), scraped 603148 items (at 6158 items/min)
2018-05-24 02:47:26 [scrapy.extensions.logstats] INFO: Crawled 909680 pages (at 6933 pages/min), scraped 609661 items (at 6513 items/min)
2018-05-24 02:48:26 [scrapy.extensions.logstats] INFO: Crawled 916768 pages (at 7088 pages/min), scraped 622775 items (at 13114 items/min)
2018-05-24 02:49:26 [scrapy.extensions.logstats] INFO: Crawled 922924 pages (at 6156 pages/min), scraped 643670 items (at 20895 items/min)
2018-05-24 02:50:26 [scrapy.extensions.logstats] INFO: Crawled 929425 pages (at 6501 pages/min), scraped 648656 items (at 4986 items/min)
2018-05-24 02:51:26 [scrapy.extensions.logstats] INFO: Crawled 935835 pages (at 6410 pages/min), scraped 652550 items (at 3894 items/min)
2018-05-24 02:52:26 [scrapy.extensions.logstats] INFO: Crawled 942701 pages (at 6866 pages/min), scraped 656428 items (at 3878 items/min)
2018-05-24 02:53:26 [scrapy.extensions.logstats] INFO: Crawled 949487 pages (at 6786 pages/min), scraped 659769 items (at 3341 items/min)
2018-05-24 02:54:26 [scrapy.extensions.logstats] INFO: Crawled 957208 pages (at 7721 pages/min), scraped 665610 items (at 5841 items/min)
2018-05-24 02:55:26 [scrapy.extensions.logstats] INFO: Crawled 964260 pages (at 7052 pages/min), scraped 669869 items (at 4259 items/min)
2018-05-24 02:56:26 [scrapy.extensions.logstats] INFO: Crawled 972185 pages (at 7925 pages/min), scraped 681322 items (at 11453 items/min)
2018-05-24 02:57:26 [scrapy.extensions.logstats] INFO: Crawled 979930 pages (at 7745 pages/min), scraped 694857 items (at 13535 items/min)
2018-05-24 02:58:26 [scrapy.extensions.logstats] INFO: Crawled 987536 pages (at 7606 pages/min), scraped 703663 items (at 8806 items/min)
2018-05-24 02:59:26 [scrapy.extensions.logstats] INFO: Crawled 994739 pages (at 7203 pages/min), scraped 713501 items (at 9838 items/min)
2018-05-24 03:00:26 [scrapy.extensions.logstats] INFO: Crawled 1002106 pages (at 7367 pages/min), scraped 722970 items (at 9469 items/min)
2018-05-24 03:01:26 [scrapy.extensions.logstats] INFO: Crawled 1009009 pages (at 6903 pages/min), scraped 729307 items (at 6337 items/min)
2018-05-24 03:02:26 [scrapy.extensions.logstats] INFO: Crawled 1016295 pages (at 7286 pages/min), scraped 734448 items (at 5141 items/min)
2018-05-24 03:03:26 [scrapy.extensions.logstats] INFO: Crawled 1019618 pages (at 3323 pages/min), scraped 737019 items (at 2571 items/min)
2018-05-24 03:04:26 [scrapy.extensions.logstats] INFO: Crawled 1025755 pages (at 6137 pages/min), scraped 743117 items (at 6098 items/min)
2018-05-24 03:05:26 [scrapy.extensions.logstats] INFO: Crawled 1033190 pages (at 7435 pages/min), scraped 754039 items (at 10922 items/min)
2018-05-24 03:06:26 [scrapy.extensions.logstats] INFO: Crawled 1040499 pages (at 7309 pages/min), scraped 758214 items (at 4175 items/min)
2018-05-24 03:07:26 [scrapy.extensions.logstats] INFO: Crawled 1048123 pages (at 7624 pages/min), scraped 764556 items (at 6342 items/min)
2018-05-24 03:08:26 [scrapy.extensions.logstats] INFO: Crawled 1054832 pages (at 6709 pages/min), scraped 766278 items (at 1722 items/min)
2018-05-24 03:09:26 [scrapy.extensions.logstats] INFO: Crawled 1061993 pages (at 7161 pages/min), scraped 767840 items (at 1562 items/min)
2018-05-24 03:10:26 [scrapy.extensions.logstats] INFO: Crawled 1069411 pages (at 7418 pages/min), scraped 776765 items (at 8925 items/min)
2018-05-24 03:11:26 [scrapy.extensions.logstats] INFO: Crawled 1076892 pages (at 7481 pages/min), scraped 784407 items (at 7642 items/min)
2018-05-24 03:12:26 [scrapy.extensions.logstats] INFO: Crawled 1084130 pages (at 7238 pages/min), scraped 786290 items (at 1883 items/min)
2018-05-24 03:13:26 [scrapy.extensions.logstats] INFO: Crawled 1091175 pages (at 7045 pages/min), scraped 794585 items (at 8295 items/min)
2018-05-24 03:14:26 [scrapy.extensions.logstats] INFO: Crawled 1098556 pages (at 7381 pages/min), scraped 804750 items (at 10165 items/min)
2018-05-24 03:15:26 [scrapy.extensions.logstats] INFO: Crawled 1106114 pages (at 7558 pages/min), scraped 819109 items (at 14359 items/min)
2018-05-24 03:16:26 [scrapy.extensions.logstats] INFO: Crawled 1112029 pages (at 5915 pages/min), scraped 839051 items (at 19942 items/min)
2018-05-24 03:17:26 [scrapy.extensions.logstats] INFO: Crawled 1117771 pages (at 5742 pages/min), scraped 861518 items (at 22467 items/min)
2018-05-24 03:18:26 [scrapy.extensions.logstats] INFO: Crawled 1122575 pages (at 4804 pages/min), scraped 881934 items (at 20416 items/min)
2018-05-24 03:19:26 [scrapy.extensions.logstats] INFO: Crawled 1127887 pages (at 5312 pages/min), scraped 887941 items (at 6007 items/min)
2018-05-24 03:20:26 [scrapy.extensions.logstats] INFO: Crawled 1135080 pages (at 7193 pages/min), scraped 888625 items (at 684 items/min)
2018-05-24 03:21:26 [scrapy.extensions.logstats] INFO: Crawled 1142231 pages (at 7151 pages/min), scraped 889280 items (at 655 items/min)
2018-05-24 03:22:26 [scrapy.extensions.logstats] INFO: Crawled 1148073 pages (at 5842 pages/min), scraped 897262 items (at 7982 items/min)
2018-05-24 03:23:26 [scrapy.extensions.logstats] INFO: Crawled 1152615 pages (at 4542 pages/min), scraped 921740 items (at 24478 items/min)
2018-05-24 03:24:26 [scrapy.extensions.logstats] INFO: Crawled 1157655 pages (at 5040 pages/min), scraped 929880 items (at 8140 items/min)
2018-05-24 03:25:26 [scrapy.extensions.logstats] INFO: Crawled 1163923 pages (at 6268 pages/min), scraped 933331 items (at 3451 items/min)
2018-05-24 03:26:26 [scrapy.extensions.logstats] INFO: Crawled 1170033 pages (at 6110 pages/min), scraped 937318 items (at 3987 items/min)
2018-05-24 03:27:26 [scrapy.extensions.logstats] INFO: Crawled 1177091 pages (at 7058 pages/min), scraped 942771 items (at 5453 items/min)
2018-05-24 03:28:26 [scrapy.extensions.logstats] INFO: Crawled 1183613 pages (at 6522 pages/min), scraped 949149 items (at 6378 items/min)
2018-05-24 03:29:26 [scrapy.extensions.logstats] INFO: Crawled 1191132 pages (at 7519 pages/min), scraped 964659 items (at 15510 items/min)
2018-05-24 03:30:26 [scrapy.extensions.logstats] INFO: Crawled 1198673 pages (at 7541 pages/min), scraped 976580 items (at 11921 items/min)
2018-05-24 03:31:26 [scrapy.extensions.logstats] INFO: Crawled 1206060 pages (at 7387 pages/min), scraped 996177 items (at 19597 items/min)
2018-05-24 03:32:26 [scrapy.extensions.logstats] INFO: Crawled 1211639 pages (at 5579 pages/min), scraped 1017894 items (at 21717 items/min)
2018-05-24 03:33:26 [scrapy.extensions.logstats] INFO: Crawled 1218513 pages (at 6874 pages/min), scraped 1024229 items (at 6335 items/min)
2018-05-24 03:34:26 [scrapy.extensions.logstats] INFO: Crawled 1225721 pages (at 7208 pages/min), scraped 1024885 items (at 656 items/min)
2018-05-24 03:35:26 [scrapy.extensions.logstats] INFO: Crawled 1232712 pages (at 6991 pages/min), scraped 1025557 items (at 672 items/min)
2018-05-24 03:36:26 [scrapy.extensions.logstats] INFO: Crawled 1239499 pages (at 6787 pages/min), scraped 1026942 items (at 1385 items/min)
2018-05-24 03:37:26 [scrapy.extensions.logstats] INFO: Crawled 1246911 pages (at 7412 pages/min), scraped 1033366 items (at 6424 items/min)
2018-05-24 03:38:26 [scrapy.extensions.logstats] INFO: Crawled 1253957 pages (at 7046 pages/min), scraped 1038514 items (at 5148 items/min)
2018-05-24 03:39:26 [scrapy.extensions.logstats] INFO: Crawled 1261303 pages (at 7346 pages/min), scraped 1045924 items (at 7410 items/min)
2018-05-24 03:40:26 [scrapy.extensions.logstats] INFO: Crawled 1268512 pages (at 7209 pages/min), scraped 1049971 items (at 4047 items/min)
2018-05-24 03:41:26 [scrapy.extensions.logstats] INFO: Crawled 1275733 pages (at 7221 pages/min), scraped 1054107 items (at 4136 items/min)
2018-05-24 03:42:26 [scrapy.extensions.logstats] INFO: Crawled 1282701 pages (at 6968 pages/min), scraped 1056681 items (at 2574 items/min)
2018-05-24 03:43:26 [scrapy.extensions.logstats] INFO: Crawled 1289497 pages (at 6796 pages/min), scraped 1058214 items (at 1533 items/min)
2018-05-24 03:44:26 [scrapy.extensions.logstats] INFO: Crawled 1296628 pages (at 7131 pages/min), scraped 1066346 items (at 8132 items/min)
2018-05-24 03:45:26 [scrapy.extensions.logstats] INFO: Crawled 1303899 pages (at 7271 pages/min), scraped 1079438 items (at 13092 items/min)
2018-05-24 03:46:26 [scrapy.extensions.logstats] INFO: Crawled 1310998 pages (at 7099 pages/min), scraped 1090304 items (at 10866 items/min)
2018-05-24 03:47:26 [scrapy.extensions.logstats] INFO: Crawled 1317894 pages (at 6896 pages/min), scraped 1091189 items (at 885 items/min)
2018-05-24 03:48:26 [scrapy.extensions.logstats] INFO: Crawled 1324985 pages (at 7091 pages/min), scraped 1103192 items (at 12003 items/min)
2018-05-24 03:49:26 [scrapy.extensions.logstats] INFO: Crawled 1332153 pages (at 7168 pages/min), scraped 1115768 items (at 12576 items/min)
2018-05-24 03:50:26 [scrapy.extensions.logstats] INFO: Crawled 1338323 pages (at 6170 pages/min), scraped 1143515 items (at 27747 items/min)
2018-05-24 03:51:26 [scrapy.extensions.logstats] INFO: Crawled 1344944 pages (at 6621 pages/min), scraped 1155921 items (at 12406 items/min)
2018-05-24 03:52:26 [scrapy.extensions.logstats] INFO: Crawled 1351262 pages (at 6318 pages/min), scraped 1157495 items (at 1574 items/min)
2018-05-24 03:53:26 [scrapy.extensions.logstats] INFO: Crawled 1356933 pages (at 5671 pages/min), scraped 1166190 items (at 8695 items/min)
2018-05-24 03:54:26 [scrapy.extensions.logstats] INFO: Crawled 1364112 pages (at 7179 pages/min), scraped 1172905 items (at 6715 items/min)
2018-05-24 03:55:26 [scrapy.extensions.logstats] INFO: Crawled 1371545 pages (at 7433 pages/min), scraped 1192087 items (at 19182 items/min)
2018-05-24 03:56:26 [scrapy.extensions.logstats] INFO: Crawled 1378741 pages (at 7196 pages/min), scraped 1208136 items (at 16049 items/min)
2018-05-24 03:57:26 [scrapy.extensions.logstats] INFO: Crawled 1386011 pages (at 7270 pages/min), scraped 1225745 items (at 17609 items/min)
2018-05-24 03:58:26 [scrapy.extensions.logstats] INFO: Crawled 1391597 pages (at 5586 pages/min), scraped 1255464 items (at 29719 items/min)
2018-05-24 03:59:26 [scrapy.extensions.logstats] INFO: Crawled 1397358 pages (at 5761 pages/min), scraped 1276186 items (at 20722 items/min)
2018-05-24 04:00:26 [scrapy.extensions.logstats] INFO: Crawled 1404479 pages (at 7121 pages/min), scraped 1283231 items (at 7045 items/min)
2018-05-24 04:01:26 [scrapy.extensions.logstats] INFO: Crawled 1411572 pages (at 7093 pages/min), scraped 1285672 items (at 2441 items/min)
2018-05-24 04:02:26 [scrapy.extensions.logstats] INFO: Crawled 1416687 pages (at 5115 pages/min), scraped 1287707 items (at 2035 items/min)
2018-05-24 04:03:26 [scrapy.extensions.logstats] INFO: Crawled 1423394 pages (at 6707 pages/min), scraped 1293324 items (at 5617 items/min)
2018-05-24 04:04:26 [scrapy.extensions.logstats] INFO: Crawled 1430557 pages (at 7163 pages/min), scraped 1294152 items (at 828 items/min)
2018-05-24 04:05:26 [scrapy.extensions.logstats] INFO: Crawled 1437679 pages (at 7122 pages/min), scraped 1294888 items (at 736 items/min)
2018-05-24 04:06:26 [scrapy.extensions.logstats] INFO: Crawled 1444886 pages (at 7207 pages/min), scraped 1295877 items (at 989 items/min)
2018-05-24 04:07:26 [scrapy.extensions.logstats] INFO: Crawled 1451907 pages (at 7021 pages/min), scraped 1297333 items (at 1456 items/min)
2018-05-24 04:08:26 [scrapy.extensions.logstats] INFO: Crawled 1459157 pages (at 7250 pages/min), scraped 1299832 items (at 2499 items/min)
2018-05-24 04:09:26 [scrapy.extensions.logstats] INFO: Crawled 1466369 pages (at 7212 pages/min), scraped 1307108 items (at 7276 items/min)
2018-05-24 04:10:26 [scrapy.extensions.logstats] INFO: Crawled 1473358 pages (at 6989 pages/min), scraped 1308468 items (at 1360 items/min)
2018-05-24 04:11:26 [scrapy.extensions.logstats] INFO: Crawled 1480668 pages (at 7310 pages/min), scraped 1309453 items (at 985 items/min)
2018-05-24 04:12:26 [scrapy.extensions.logstats] INFO: Crawled 1487790 pages (at 7122 pages/min), scraped 1314728 items (at 5275 items/min)
2018-05-24 04:13:26 [scrapy.extensions.logstats] INFO: Crawled 1494550 pages (at 6760 pages/min), scraped 1319752 items (at 5024 items/min)
2018-05-24 04:14:26 [scrapy.extensions.logstats] INFO: Crawled 1500524 pages (at 5974 pages/min), scraped 1324012 items (at 4260 items/min)
2018-05-24 04:15:26 [scrapy.extensions.logstats] INFO: Crawled 1507945 pages (at 7421 pages/min), scraped 1331843 items (at 7831 items/min)
2018-05-24 04:16:26 [scrapy.extensions.logstats] INFO: Crawled 1514664 pages (at 6719 pages/min), scraped 1351655 items (at 19812 items/min)
2018-05-24 04:17:26 [scrapy.extensions.logstats] INFO: Crawled 1519817 pages (at 5153 pages/min), scraped 1366754 items (at 15099 items/min)
2018-05-24 04:18:26 [scrapy.extensions.logstats] INFO: Crawled 1524436 pages (at 4619 pages/min), scraped 1377522 items (at 10768 items/min)
2018-05-24 04:19:26 [scrapy.extensions.logstats] INFO: Crawled 1529910 pages (at 5474 pages/min), scraped 1397910 items (at 20388 items/min)
2018-05-24 04:20:26 [scrapy.extensions.logstats] INFO: Crawled 1536196 pages (at 6286 pages/min), scraped 1399739 items (at 1829 items/min)
2018-05-24 04:21:26 [scrapy.extensions.logstats] INFO: Crawled 1542614 pages (at 6418 pages/min), scraped 1404073 items (at 4334 items/min)
2018-05-24 04:22:26 [scrapy.extensions.logstats] INFO: Crawled 1549051 pages (at 6437 pages/min), scraped 1412009 items (at 7936 items/min)
2018-05-24 04:23:26 [scrapy.extensions.logstats] INFO: Crawled 1556266 pages (at 7215 pages/min), scraped 1422312 items (at 10303 items/min)
2018-05-24 04:24:26 [scrapy.extensions.logstats] INFO: Crawled 1563530 pages (at 7264 pages/min), scraped 1436129 items (at 13817 items/min)
2018-05-24 04:25:26 [scrapy.extensions.logstats] INFO: Crawled 1570958 pages (at 7428 pages/min), scraped 1451993 items (at 15864 items/min)
2018-05-24 04:26:26 [scrapy.extensions.logstats] INFO: Crawled 1577701 pages (at 6743 pages/min), scraped 1472230 items (at 20237 items/min)
2018-05-24 04:27:26 [scrapy.extensions.logstats] INFO: Crawled 1584699 pages (at 6998 pages/min), scraped 1486078 items (at 13848 items/min)
2018-05-24 04:28:26 [scrapy.extensions.logstats] INFO: Crawled 1591781 pages (at 7082 pages/min), scraped 1487613 items (at 1535 items/min)
2018-05-24 04:29:26 [scrapy.extensions.logstats] INFO: Crawled 1598747 pages (at 6966 pages/min), scraped 1496426 items (at 8813 items/min)
2018-05-24 04:30:26 [scrapy.extensions.logstats] INFO: Crawled 1605924 pages (at 7177 pages/min), scraped 1497101 items (at 675 items/min)
2018-05-24 04:31:26 [scrapy.extensions.logstats] INFO: Crawled 1612515 pages (at 6591 pages/min), scraped 1497684 items (at 583 items/min)
2018-05-24 04:32:26 [scrapy.extensions.logstats] INFO: Crawled 1619766 pages (at 7251 pages/min), scraped 1498415 items (at 731 items/min)
2018-05-24 04:33:26 [scrapy.extensions.logstats] INFO: Crawled 1626479 pages (at 6713 pages/min), scraped 1499105 items (at 690 items/min)
2018-05-24 04:34:26 [scrapy.extensions.logstats] INFO: Crawled 1632041 pages (at 5562 pages/min), scraped 1502239 items (at 3134 items/min)
2018-05-24 04:35:26 [scrapy.extensions.logstats] INFO: Crawled 1639454 pages (at 7413 pages/min), scraped 1505712 items (at 3473 items/min)
2018-05-24 04:36:26 [scrapy.extensions.logstats] INFO: Crawled 1646979 pages (at 7525 pages/min), scraped 1509791 items (at 4079 items/min)
2018-05-24 04:37:26 [scrapy.extensions.logstats] INFO: Crawled 1654433 pages (at 7454 pages/min), scraped 1517732 items (at 7941 items/min)
2018-05-24 04:38:26 [scrapy.extensions.logstats] INFO: Crawled 1661754 pages (at 7321 pages/min), scraped 1520677 items (at 2945 items/min)
2018-05-24 04:39:26 [scrapy.extensions.logstats] INFO: Crawled 1669055 pages (at 7301 pages/min), scraped 1523829 items (at 3152 items/min)
2018-05-24 04:40:26 [scrapy.extensions.logstats] INFO: Crawled 1676114 pages (at 7059 pages/min), scraped 1533186 items (at 9357 items/min)
2018-05-24 04:41:26 [scrapy.extensions.logstats] INFO: Crawled 1682975 pages (at 6861 pages/min), scraped 1550427 items (at 17241 items/min)
2018-05-24 04:42:26 [scrapy.extensions.logstats] INFO: Crawled 1690064 pages (at 7089 pages/min), scraped 1568927 items (at 18500 items/min)
2018-05-24 04:43:26 [scrapy.extensions.logstats] INFO: Crawled 1695687 pages (at 5623 pages/min), scraped 1582447 items (at 13520 items/min)
2018-05-24 04:44:26 [scrapy.extensions.logstats] INFO: Crawled 1701396 pages (at 5709 pages/min), scraped 1595528 items (at 13081 items/min)
2018-05-24 04:45:26 [scrapy.extensions.logstats] INFO: Crawled 1707551 pages (at 6155 pages/min), scraped 1614244 items (at 18716 items/min)
2018-05-24 04:46:26 [scrapy.extensions.logstats] INFO: Crawled 1713002 pages (at 5451 pages/min), scraped 1638735 items (at 24491 items/min)
2018-05-24 04:47:26 [scrapy.extensions.logstats] INFO: Crawled 1717954 pages (at 4952 pages/min), scraped 1660615 items (at 21880 items/min)
2018-05-24 04:48:26 [scrapy.extensions.logstats] INFO: Crawled 1722547 pages (at 4593 pages/min), scraped 1679141 items (at 18526 items/min)
2018-05-24 04:49:26 [scrapy.extensions.logstats] INFO: Crawled 1726744 pages (at 4197 pages/min), scraped 1690974 items (at 11833 items/min)
2018-05-24 04:50:26 [scrapy.extensions.logstats] INFO: Crawled 1730928 pages (at 4184 pages/min), scraped 1696957 items (at 5983 items/min)
2018-05-24 04:51:26 [scrapy.extensions.logstats] INFO: Crawled 1734835 pages (at 3907 pages/min), scraped 1704397 items (at 7440 items/min)
2018-05-24 04:52:26 [scrapy.extensions.logstats] INFO: Crawled 1739105 pages (at 4270 pages/min), scraped 1707556 items (at 3159 items/min)
2018-05-24 04:53:26 [scrapy.extensions.logstats] INFO: Crawled 1743483 pages (at 4378 pages/min), scraped 1724088 items (at 16532 items/min)
2018-05-24 04:54:26 [scrapy.extensions.logstats] INFO: Crawled 1749233 pages (at 5750 pages/min), scraped 1731827 items (at 7739 items/min)
2018-05-24 04:55:26 [scrapy.extensions.logstats] INFO: Crawled 1753571 pages (at 4338 pages/min), scraped 1735625 items (at 3798 items/min)
2018-05-24 04:56:26 [scrapy.extensions.logstats] INFO: Crawled 1759970 pages (at 6399 pages/min), scraped 1742875 items (at 7250 items/min)
2018-05-24 04:57:26 [scrapy.extensions.logstats] INFO: Crawled 1766015 pages (at 6045 pages/min), scraped 1750856 items (at 7981 items/min)
2018-05-24 04:58:26 [scrapy.extensions.logstats] INFO: Crawled 1772455 pages (at 6440 pages/min), scraped 1760700 items (at 9844 items/min)
2018-05-24 04:59:26 [scrapy.extensions.logstats] INFO: Crawled 1779279 pages (at 6824 pages/min), scraped 1777784 items (at 17084 items/min)
2018-05-24 05:00:26 [scrapy.extensions.logstats] INFO: Crawled 1785843 pages (at 6564 pages/min), scraped 1801218 items (at 23434 items/min)
2018-05-24 05:01:26 [scrapy.extensions.logstats] INFO: Crawled 1792169 pages (at 6326 pages/min), scraped 1827569 items (at 26351 items/min)
2018-05-24 05:02:26 [scrapy.extensions.logstats] INFO: Crawled 1797485 pages (at 5316 pages/min), scraped 1851295 items (at 23726 items/min)
2018-05-24 05:03:26 [scrapy.extensions.logstats] INFO: Crawled 1803543 pages (at 6058 pages/min), scraped 1860399 items (at 9104 items/min)
2018-05-24 05:04:26 [scrapy.extensions.logstats] INFO: Crawled 1809795 pages (at 6252 pages/min), scraped 1874154 items (at 13755 items/min)
2018-05-24 05:05:26 [scrapy.extensions.logstats] INFO: Crawled 1816450 pages (at 6655 pages/min), scraped 1893756 items (at 19602 items/min)
2018-05-24 05:06:26 [scrapy.extensions.logstats] INFO: Crawled 1822555 pages (at 6105 pages/min), scraped 1904798 items (at 11042 items/min)
2018-05-24 05:07:26 [scrapy.extensions.logstats] INFO: Crawled 1829447 pages (at 6892 pages/min), scraped 1915009 items (at 10211 items/min)
2018-05-24 05:08:26 [scrapy.extensions.logstats] INFO: Crawled 1836749 pages (at 7302 pages/min), scraped 1921113 items (at 6104 items/min)
2018-05-24 05:09:26 [scrapy.extensions.logstats] INFO: Crawled 1843242 pages (at 6493 pages/min), scraped 1927574 items (at 6461 items/min)
2018-05-24 05:10:26 [scrapy.extensions.logstats] INFO: Crawled 1850118 pages (at 6876 pages/min), scraped 1941981 items (at 14407 items/min)
2018-05-24 05:11:26 [scrapy.extensions.logstats] INFO: Crawled 1855523 pages (at 5405 pages/min), scraped 1965437 items (at 23456 items/min)
2018-05-24 05:12:26 [scrapy.extensions.logstats] INFO: Crawled 1861313 pages (at 5790 pages/min), scraped 1993648 items (at 28211 items/min)
2018-05-24 05:13:26 [scrapy.extensions.logstats] INFO: Crawled 1867577 pages (at 6264 pages/min), scraped 2008812 items (at 15164 items/min)
2018-05-24 05:14:26 [scrapy.extensions.logstats] INFO: Crawled 1874289 pages (at 6712 pages/min), scraped 2011460 items (at 2648 items/min)
2018-05-24 05:15:26 [scrapy.extensions.logstats] INFO: Crawled 1881301 pages (at 7012 pages/min), scraped 2022248 items (at 10788 items/min)
2018-05-24 05:16:26 [scrapy.extensions.logstats] INFO: Crawled 1887466 pages (at 6165 pages/min), scraped 2039680 items (at 17432 items/min)
2018-05-24 05:17:26 [scrapy.extensions.logstats] INFO: Crawled 1893951 pages (at 6485 pages/min), scraped 2043030 items (at 3350 items/min)
2018-05-24 05:18:26 [scrapy.extensions.logstats] INFO: Crawled 1900840 pages (at 6889 pages/min), scraped 2046888 items (at 3858 items/min)
2018-05-24 05:19:26 [scrapy.extensions.logstats] INFO: Crawled 1907737 pages (at 6897 pages/min), scraped 2051474 items (at 4586 items/min)
2018-05-24 05:20:26 [scrapy.extensions.logstats] INFO: Crawled 1913415 pages (at 5678 pages/min), scraped 2057219 items (at 5745 items/min)
2018-05-24 05:21:26 [scrapy.extensions.logstats] INFO: Crawled 1920869 pages (at 7454 pages/min), scraped 2073961 items (at 16742 items/min)
2018-05-24 05:22:26 [scrapy.extensions.logstats] INFO: Crawled 1925166 pages (at 4297 pages/min), scraped 2089253 items (at 15292 items/min)
2018-05-24 05:23:26 [scrapy.extensions.logstats] INFO: Crawled 1932017 pages (at 6851 pages/min), scraped 2106576 items (at 17323 items/min)
2018-05-24 05:24:26 [scrapy.extensions.logstats] INFO: Crawled 1937040 pages (at 5023 pages/min), scraped 2133594 items (at 27018 items/min)
2018-05-24 05:25:26 [scrapy.extensions.logstats] INFO: Crawled 1942761 pages (at 5721 pages/min), scraped 2152260 items (at 18666 items/min)
2018-05-24 05:26:26 [scrapy.extensions.logstats] INFO: Crawled 1947319 pages (at 4558 pages/min), scraped 2179731 items (at 27471 items/min)
2018-05-24 05:27:26 [scrapy.extensions.logstats] INFO: Crawled 1951270 pages (at 3951 pages/min), scraped 2202066 items (at 22335 items/min)
2018-05-24 05:28:26 [scrapy.extensions.logstats] INFO: Crawled 1954811 pages (at 3541 pages/min), scraped 2220583 items (at 18517 items/min)
2018-05-24 05:29:26 [scrapy.extensions.logstats] INFO: Crawled 1958022 pages (at 3211 pages/min), scraped 2233803 items (at 13220 items/min)
2018-05-24 05:30:26 [scrapy.extensions.logstats] INFO: Crawled 1959465 pages (at 1443 pages/min), scraped 2242814 items (at 9011 items/min)
2018-05-24 05:31:26 [scrapy.extensions.logstats] INFO: Crawled 1965083 pages (at 5618 pages/min), scraped 2264658 items (at 21844 items/min)
2018-05-24 05:32:26 [scrapy.extensions.logstats] INFO: Crawled 1970029 pages (at 4946 pages/min), scraped 2276313 items (at 11655 items/min)
2018-05-24 05:33:26 [scrapy.extensions.logstats] INFO: Crawled 1975711 pages (at 5682 pages/min), scraped 2295126 items (at 18813 items/min)
2018-05-24 05:34:26 [scrapy.extensions.logstats] INFO: Crawled 1981505 pages (at 5794 pages/min), scraped 2298247 items (at 3121 items/min)
2018-05-24 05:35:26 [scrapy.extensions.logstats] INFO: Crawled 1986471 pages (at 4966 pages/min), scraped 2308539 items (at 10292 items/min)
2018-05-24 05:36:26 [scrapy.extensions.logstats] INFO: Crawled 1992088 pages (at 5617 pages/min), scraped 2328907 items (at 20368 items/min)
2018-05-24 05:37:26 [scrapy.extensions.logstats] INFO: Crawled 1997118 pages (at 5030 pages/min), scraped 2337541 items (at 8634 items/min)
2018-05-24 05:38:26 [scrapy.extensions.logstats] INFO: Crawled 2002520 pages (at 5402 pages/min), scraped 2347292 items (at 9751 items/min)
2018-05-24 05:38:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://list.jd.com/list.html?cat=6233,6279,6287&page=11&sort=sort_rank_asc&trans=1&JL=6_0_0> (referer: https://list.jd.com/list.html?cat=6233,6279,6287&page=10&sort=sort_rank_asc&trans=1&JL=6_0_0)
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\python-learning\scrapy\jd_spider\jd_spider\jd_spider\jd_spider\spiders\jd.py", line 38, in parse_goods_urls
    goods_url = 'https:' + goods_url
TypeError: Can't convert 'NoneType' object to str implicitly
2018-05-24 05:39:26 [scrapy.extensions.logstats] INFO: Crawled 2008696 pages (at 6176 pages/min), scraped 2359446 items (at 12154 items/min)
2018-05-24 05:40:26 [scrapy.extensions.logstats] INFO: Crawled 2013809 pages (at 5113 pages/min), scraped 2371496 items (at 12050 items/min)
2018-05-24 05:41:26 [scrapy.extensions.logstats] INFO: Crawled 2019139 pages (at 5330 pages/min), scraped 2372450 items (at 954 items/min)
2018-05-24 05:42:26 [scrapy.extensions.logstats] INFO: Crawled 2025152 pages (at 6013 pages/min), scraped 2373964 items (at 1514 items/min)
2018-05-24 05:43:26 [scrapy.extensions.logstats] INFO: Crawled 2030562 pages (at 5410 pages/min), scraped 2374764 items (at 800 items/min)
2018-05-24 05:44:26 [scrapy.extensions.logstats] INFO: Crawled 2037556 pages (at 6994 pages/min), scraped 2376153 items (at 1389 items/min)
2018-05-24 05:45:26 [scrapy.extensions.logstats] INFO: Crawled 2044737 pages (at 7181 pages/min), scraped 2379845 items (at 3692 items/min)
2018-05-24 05:46:26 [scrapy.extensions.logstats] INFO: Crawled 2050623 pages (at 5886 pages/min), scraped 2384610 items (at 4765 items/min)
2018-05-24 05:47:26 [scrapy.extensions.logstats] INFO: Crawled 2057278 pages (at 6655 pages/min), scraped 2387999 items (at 3389 items/min)
2018-05-24 05:48:26 [scrapy.extensions.logstats] INFO: Crawled 2062134 pages (at 4856 pages/min), scraped 2389112 items (at 1113 items/min)
2018-05-24 05:49:26 [scrapy.extensions.logstats] INFO: Crawled 2069186 pages (at 7052 pages/min), scraped 2396466 items (at 7354 items/min)
2018-05-24 05:50:26 [scrapy.extensions.logstats] INFO: Crawled 2075412 pages (at 6226 pages/min), scraped 2399949 items (at 3483 items/min)
2018-05-24 05:51:26 [scrapy.extensions.logstats] INFO: Crawled 2080868 pages (at 5456 pages/min), scraped 2402866 items (at 2917 items/min)
2018-05-24 05:52:26 [scrapy.extensions.logstats] INFO: Crawled 2087432 pages (at 6564 pages/min), scraped 2403950 items (at 1084 items/min)
2018-05-24 05:53:26 [scrapy.extensions.logstats] INFO: Crawled 2093438 pages (at 6006 pages/min), scraped 2405752 items (at 1802 items/min)
2018-05-24 05:54:26 [scrapy.extensions.logstats] INFO: Crawled 2100352 pages (at 6914 pages/min), scraped 2407221 items (at 1469 items/min)
2018-05-24 05:55:26 [scrapy.extensions.logstats] INFO: Crawled 2107258 pages (at 6906 pages/min), scraped 2409120 items (at 1899 items/min)
2018-05-24 05:56:26 [scrapy.extensions.logstats] INFO: Crawled 2113548 pages (at 6290 pages/min), scraped 2412222 items (at 3102 items/min)
2018-05-24 05:57:26 [scrapy.extensions.logstats] INFO: Crawled 2120161 pages (at 6613 pages/min), scraped 2418459 items (at 6237 items/min)
2018-05-24 05:58:26 [scrapy.extensions.logstats] INFO: Crawled 2125646 pages (at 5485 pages/min), scraped 2420808 items (at 2349 items/min)
2018-05-24 05:59:26 [scrapy.extensions.logstats] INFO: Crawled 2131221 pages (at 5575 pages/min), scraped 2429667 items (at 8859 items/min)
2018-05-24 06:00:26 [scrapy.extensions.logstats] INFO: Crawled 2136957 pages (at 5736 pages/min), scraped 2445532 items (at 15865 items/min)
2018-05-24 06:01:26 [scrapy.extensions.logstats] INFO: Crawled 2142897 pages (at 5940 pages/min), scraped 2449546 items (at 4014 items/min)
2018-05-24 06:02:26 [scrapy.extensions.logstats] INFO: Crawled 2149912 pages (at 7015 pages/min), scraped 2450540 items (at 994 items/min)
2018-05-24 06:03:26 [scrapy.extensions.logstats] INFO: Crawled 2156158 pages (at 6246 pages/min), scraped 2451634 items (at 1094 items/min)
2018-05-24 06:04:26 [scrapy.extensions.logstats] INFO: Crawled 2161549 pages (at 5391 pages/min), scraped 2452425 items (at 791 items/min)
2018-05-24 06:05:26 [scrapy.extensions.logstats] INFO: Crawled 2167665 pages (at 6116 pages/min), scraped 2453358 items (at 933 items/min)
2018-05-24 06:06:26 [scrapy.extensions.logstats] INFO: Crawled 2174806 pages (at 7141 pages/min), scraped 2454638 items (at 1280 items/min)
2018-05-24 06:07:26 [scrapy.extensions.logstats] INFO: Crawled 2181317 pages (at 6511 pages/min), scraped 2458105 items (at 3467 items/min)
2018-05-24 06:08:26 [scrapy.extensions.logstats] INFO: Crawled 2188504 pages (at 7187 pages/min), scraped 2461931 items (at 3826 items/min)
2018-05-24 06:09:26 [scrapy.extensions.logstats] INFO: Crawled 2194454 pages (at 5950 pages/min), scraped 2468340 items (at 6409 items/min)
2018-05-24 06:10:26 [scrapy.extensions.logstats] INFO: Crawled 2200706 pages (at 6252 pages/min), scraped 2469504 items (at 1164 items/min)
2018-05-24 06:11:26 [scrapy.extensions.logstats] INFO: Crawled 2206548 pages (at 5842 pages/min), scraped 2472192 items (at 2688 items/min)
2018-05-24 06:12:26 [scrapy.extensions.logstats] INFO: Crawled 2212908 pages (at 6360 pages/min), scraped 2474839 items (at 2647 items/min)
2018-05-24 06:13:26 [scrapy.extensions.logstats] INFO: Crawled 2219523 pages (at 6615 pages/min), scraped 2484049 items (at 9210 items/min)
2018-05-24 06:14:26 [scrapy.extensions.logstats] INFO: Crawled 2224453 pages (at 4930 pages/min), scraped 2489741 items (at 5692 items/min)
2018-05-24 06:15:26 [scrapy.extensions.logstats] INFO: Crawled 2232276 pages (at 7823 pages/min), scraped 2504958 items (at 15217 items/min)
2018-05-24 06:16:26 [scrapy.extensions.logstats] INFO: Crawled 2238081 pages (at 5805 pages/min), scraped 2505821 items (at 863 items/min)
2018-05-24 06:17:26 [scrapy.extensions.logstats] INFO: Crawled 2245135 pages (at 7054 pages/min), scraped 2507387 items (at 1566 items/min)
2018-05-24 06:18:26 [scrapy.extensions.logstats] INFO: Crawled 2251303 pages (at 6168 pages/min), scraped 2508526 items (at 1139 items/min)
2018-05-24 06:19:26 [scrapy.extensions.logstats] INFO: Crawled 2258339 pages (at 7036 pages/min), scraped 2509547 items (at 1021 items/min)
2018-05-24 06:20:26 [scrapy.extensions.logstats] INFO: Crawled 2265593 pages (at 7254 pages/min), scraped 2511664 items (at 2117 items/min)
2018-05-24 06:21:26 [scrapy.extensions.logstats] INFO: Crawled 2272590 pages (at 6997 pages/min), scraped 2514573 items (at 2909 items/min)
2018-05-24 06:22:26 [scrapy.extensions.logstats] INFO: Crawled 2279949 pages (at 7359 pages/min), scraped 2515788 items (at 1215 items/min)
2018-05-24 06:23:26 [scrapy.extensions.logstats] INFO: Crawled 2286403 pages (at 6454 pages/min), scraped 2516723 items (at 935 items/min)
2018-05-24 06:24:26 [scrapy.extensions.logstats] INFO: Crawled 2293949 pages (at 7546 pages/min), scraped 2519542 items (at 2819 items/min)
2018-05-24 06:25:26 [scrapy.extensions.logstats] INFO: Crawled 2301166 pages (at 7217 pages/min), scraped 2522416 items (at 2874 items/min)
2018-05-24 06:26:26 [scrapy.extensions.logstats] INFO: Crawled 2308046 pages (at 6880 pages/min), scraped 2547166 items (at 24750 items/min)
2018-05-24 06:27:26 [scrapy.extensions.logstats] INFO: Crawled 2313760 pages (at 5714 pages/min), scraped 2554323 items (at 7157 items/min)
2018-05-24 06:28:26 [scrapy.extensions.logstats] INFO: Crawled 2320003 pages (at 6243 pages/min), scraped 2556380 items (at 2057 items/min)
2018-05-24 06:29:26 [scrapy.extensions.logstats] INFO: Crawled 2326116 pages (at 6113 pages/min), scraped 2561031 items (at 4651 items/min)
2018-05-24 06:30:26 [scrapy.extensions.logstats] INFO: Crawled 2332768 pages (at 6652 pages/min), scraped 2563282 items (at 2251 items/min)
2018-05-24 06:31:26 [scrapy.extensions.logstats] INFO: Crawled 2339177 pages (at 6409 pages/min), scraped 2564240 items (at 958 items/min)
2018-05-24 06:32:26 [scrapy.extensions.logstats] INFO: Crawled 2346201 pages (at 7024 pages/min), scraped 2570637 items (at 6397 items/min)
2018-05-24 06:33:26 [scrapy.extensions.logstats] INFO: Crawled 2353469 pages (at 7268 pages/min), scraped 2585940 items (at 15303 items/min)
2018-05-24 06:34:26 [scrapy.extensions.logstats] INFO: Crawled 2360909 pages (at 7440 pages/min), scraped 2605970 items (at 20030 items/min)
2018-05-24 06:35:26 [scrapy.extensions.logstats] INFO: Crawled 2368510 pages (at 7601 pages/min), scraped 2619761 items (at 13791 items/min)
2018-05-24 06:36:26 [scrapy.extensions.logstats] INFO: Crawled 2375449 pages (at 6939 pages/min), scraped 2620900 items (at 1139 items/min)
2018-05-24 06:37:26 [scrapy.extensions.logstats] INFO: Crawled 2382978 pages (at 7529 pages/min), scraped 2622700 items (at 1800 items/min)
2018-05-24 06:38:26 [scrapy.extensions.logstats] INFO: Crawled 2389648 pages (at 6670 pages/min), scraped 2624818 items (at 2118 items/min)
2018-05-24 06:39:26 [scrapy.extensions.logstats] INFO: Crawled 2397034 pages (at 7386 pages/min), scraped 2626511 items (at 1693 items/min)
2018-05-24 06:40:26 [scrapy.extensions.logstats] INFO: Crawled 2404209 pages (at 7175 pages/min), scraped 2630170 items (at 3659 items/min)
2018-05-24 06:41:26 [scrapy.extensions.logstats] INFO: Crawled 2411576 pages (at 7367 pages/min), scraped 2631524 items (at 1354 items/min)
2018-05-24 06:42:26 [scrapy.extensions.logstats] INFO: Crawled 2418533 pages (at 6957 pages/min), scraped 2634054 items (at 2530 items/min)
2018-05-24 06:43:26 [scrapy.extensions.logstats] INFO: Crawled 2425621 pages (at 7088 pages/min), scraped 2638658 items (at 4604 items/min)
2018-05-24 06:44:26 [scrapy.extensions.logstats] INFO: Crawled 2433403 pages (at 7782 pages/min), scraped 2643202 items (at 4544 items/min)
2018-05-24 06:45:26 [scrapy.extensions.logstats] INFO: Crawled 2441442 pages (at 8039 pages/min), scraped 2661979 items (at 18777 items/min)
2018-05-24 06:46:26 [scrapy.extensions.logstats] INFO: Crawled 2447419 pages (at 5977 pages/min), scraped 2677565 items (at 15586 items/min)
2018-05-24 06:47:26 [scrapy.extensions.logstats] INFO: Crawled 2453194 pages (at 5775 pages/min), scraped 2690023 items (at 12458 items/min)
2018-05-24 06:48:26 [scrapy.extensions.logstats] INFO: Crawled 2458712 pages (at 5518 pages/min), scraped 2707305 items (at 17282 items/min)
2018-05-24 06:49:26 [scrapy.extensions.logstats] INFO: Crawled 2464929 pages (at 6217 pages/min), scraped 2717275 items (at 9970 items/min)
2018-05-24 06:50:26 [scrapy.extensions.logstats] INFO: Crawled 2471742 pages (at 6813 pages/min), scraped 2724078 items (at 6803 items/min)
2018-05-24 06:51:26 [scrapy.extensions.logstats] INFO: Crawled 2478573 pages (at 6831 pages/min), scraped 2734887 items (at 10809 items/min)
2018-05-24 06:52:26 [scrapy.extensions.logstats] INFO: Crawled 2485098 pages (at 6525 pages/min), scraped 2754659 items (at 19772 items/min)
2018-05-24 06:53:26 [scrapy.extensions.logstats] INFO: Crawled 2490751 pages (at 5653 pages/min), scraped 2777414 items (at 22755 items/min)
2018-05-24 06:54:26 [scrapy.extensions.logstats] INFO: Crawled 2497275 pages (at 6524 pages/min), scraped 2795211 items (at 17797 items/min)
2018-05-24 06:55:26 [scrapy.extensions.logstats] INFO: Crawled 2503614 pages (at 6339 pages/min), scraped 2828955 items (at 33744 items/min)
2018-05-24 06:56:26 [scrapy.extensions.logstats] INFO: Crawled 2510678 pages (at 7064 pages/min), scraped 2845515 items (at 16560 items/min)
2018-05-24 06:57:26 [scrapy.extensions.logstats] INFO: Crawled 2517369 pages (at 6691 pages/min), scraped 2848018 items (at 2503 items/min)
2018-05-24 06:58:26 [scrapy.extensions.logstats] INFO: Crawled 2524438 pages (at 7069 pages/min), scraped 2859702 items (at 11684 items/min)
2018-05-24 06:59:26 [scrapy.extensions.logstats] INFO: Crawled 2530594 pages (at 6156 pages/min), scraped 2867924 items (at 8222 items/min)
2018-05-24 07:00:26 [scrapy.extensions.logstats] INFO: Crawled 2537330 pages (at 6736 pages/min), scraped 2878038 items (at 10114 items/min)
2018-05-24 07:01:26 [scrapy.extensions.logstats] INFO: Crawled 2544818 pages (at 7488 pages/min), scraped 2884300 items (at 6262 items/min)
2018-05-24 07:02:26 [scrapy.extensions.logstats] INFO: Crawled 2551856 pages (at 7038 pages/min), scraped 2899279 items (at 14979 items/min)
2018-05-24 07:03:26 [scrapy.extensions.logstats] INFO: Crawled 2557878 pages (at 6022 pages/min), scraped 2916248 items (at 16969 items/min)
2018-05-24 07:04:26 [scrapy.extensions.logstats] INFO: Crawled 2564343 pages (at 6465 pages/min), scraped 2933111 items (at 16863 items/min)
2018-05-24 07:05:26 [scrapy.extensions.logstats] INFO: Crawled 2572088 pages (at 7745 pages/min), scraped 2942656 items (at 9545 items/min)
2018-05-24 07:06:26 [scrapy.extensions.logstats] INFO: Crawled 2579190 pages (at 7102 pages/min), scraped 2949222 items (at 6566 items/min)
2018-05-24 07:07:26 [scrapy.extensions.logstats] INFO: Crawled 2585684 pages (at 6494 pages/min), scraped 2962436 items (at 13214 items/min)
2018-05-24 07:08:26 [scrapy.extensions.logstats] INFO: Crawled 2592395 pages (at 6711 pages/min), scraped 2977835 items (at 15399 items/min)
2018-05-24 07:09:26 [scrapy.extensions.logstats] INFO: Crawled 2598291 pages (at 5896 pages/min), scraped 3002386 items (at 24551 items/min)
2018-05-24 07:10:26 [scrapy.extensions.logstats] INFO: Crawled 2603115 pages (at 4824 pages/min), scraped 3011817 items (at 9431 items/min)
2018-05-24 07:11:26 [scrapy.extensions.logstats] INFO: Crawled 2609249 pages (at 6134 pages/min), scraped 3021200 items (at 9383 items/min)
2018-05-24 07:12:26 [scrapy.extensions.logstats] INFO: Crawled 2615009 pages (at 5760 pages/min), scraped 3029095 items (at 7895 items/min)
2018-05-24 07:13:26 [scrapy.extensions.logstats] INFO: Crawled 2621612 pages (at 6603 pages/min), scraped 3039531 items (at 10436 items/min)
2018-05-24 07:14:26 [scrapy.extensions.logstats] INFO: Crawled 2628709 pages (at 7097 pages/min), scraped 3045935 items (at 6404 items/min)
2018-05-24 07:15:26 [scrapy.extensions.logstats] INFO: Crawled 2634576 pages (at 5867 pages/min), scraped 3050765 items (at 4830 items/min)
2018-05-24 07:16:26 [scrapy.extensions.logstats] INFO: Crawled 2641821 pages (at 7245 pages/min), scraped 3063187 items (at 12422 items/min)
2018-05-24 07:17:26 [scrapy.extensions.logstats] INFO: Crawled 2649602 pages (at 7781 pages/min), scraped 3082947 items (at 19760 items/min)
2018-05-24 07:18:26 [scrapy.extensions.logstats] INFO: Crawled 2656765 pages (at 7163 pages/min), scraped 3098640 items (at 15693 items/min)
2018-05-24 07:19:26 [scrapy.extensions.logstats] INFO: Crawled 2664091 pages (at 7326 pages/min), scraped 3103356 items (at 4716 items/min)
2018-05-24 07:20:26 [scrapy.extensions.logstats] INFO: Crawled 2670563 pages (at 6472 pages/min), scraped 3106065 items (at 2709 items/min)
2018-05-24 07:21:26 [scrapy.extensions.logstats] INFO: Crawled 2677918 pages (at 7355 pages/min), scraped 3110673 items (at 4608 items/min)
2018-05-24 07:22:26 [scrapy.extensions.logstats] INFO: Crawled 2684928 pages (at 7010 pages/min), scraped 3112759 items (at 2086 items/min)
2018-05-24 07:23:26 [scrapy.extensions.logstats] INFO: Crawled 2692319 pages (at 7391 pages/min), scraped 3123959 items (at 11200 items/min)
2018-05-24 07:24:26 [scrapy.extensions.logstats] INFO: Crawled 2699312 pages (at 6993 pages/min), scraped 3127471 items (at 3512 items/min)
2018-05-24 07:25:26 [scrapy.extensions.logstats] INFO: Crawled 2706846 pages (at 7534 pages/min), scraped 3135817 items (at 8346 items/min)
2018-05-24 07:26:26 [scrapy.extensions.logstats] INFO: Crawled 2712565 pages (at 5719 pages/min), scraped 3140445 items (at 4628 items/min)
2018-05-24 07:27:26 [scrapy.extensions.logstats] INFO: Crawled 2719656 pages (at 7091 pages/min), scraped 3145993 items (at 5548 items/min)
2018-05-24 07:28:26 [scrapy.extensions.logstats] INFO: Crawled 2726963 pages (at 7307 pages/min), scraped 3167296 items (at 21303 items/min)
2018-05-24 07:29:26 [scrapy.extensions.logstats] INFO: Crawled 2732304 pages (at 5341 pages/min), scraped 3177038 items (at 9742 items/min)
2018-05-24 07:30:26 [scrapy.extensions.logstats] INFO: Crawled 2737565 pages (at 5261 pages/min), scraped 3190899 items (at 13861 items/min)
2018-05-24 07:31:26 [scrapy.extensions.logstats] INFO: Crawled 2744158 pages (at 6593 pages/min), scraped 3194356 items (at 3457 items/min)
2018-05-24 07:32:26 [scrapy.extensions.logstats] INFO: Crawled 2750735 pages (at 6577 pages/min), scraped 3195586 items (at 1230 items/min)
2018-05-24 07:33:26 [scrapy.extensions.logstats] INFO: Crawled 2757963 pages (at 7228 pages/min), scraped 3197392 items (at 1806 items/min)
2018-05-24 07:34:26 [scrapy.extensions.logstats] INFO: Crawled 2764698 pages (at 6735 pages/min), scraped 3199049 items (at 1657 items/min)
2018-05-24 07:35:26 [scrapy.extensions.logstats] INFO: Crawled 2772034 pages (at 7336 pages/min), scraped 3202170 items (at 3121 items/min)
2018-05-24 07:36:26 [scrapy.extensions.logstats] INFO: Crawled 2780200 pages (at 8166 pages/min), scraped 3212603 items (at 10433 items/min)
2018-05-24 07:37:26 [scrapy.extensions.logstats] INFO: Crawled 2786797 pages (at 6597 pages/min), scraped 3213623 items (at 1020 items/min)
2018-05-24 07:38:26 [scrapy.extensions.logstats] INFO: Crawled 2794516 pages (at 7719 pages/min), scraped 3219972 items (at 6349 items/min)
2018-05-24 07:39:26 [scrapy.extensions.logstats] INFO: Crawled 2801658 pages (at 7142 pages/min), scraped 3237394 items (at 17422 items/min)
2018-05-24 07:40:26 [scrapy.extensions.logstats] INFO: Crawled 2808565 pages (at 6907 pages/min), scraped 3251314 items (at 13920 items/min)
2018-05-24 07:40:56 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_28111038774&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527118232427. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2018-05-24 07:41:26 [scrapy.extensions.logstats] INFO: Crawled 2815495 pages (at 6930 pages/min), scraped 3258592 items (at 7278 items/min)
2018-05-24 07:42:26 [scrapy.extensions.logstats] INFO: Crawled 2822779 pages (at 7284 pages/min), scraped 3273288 items (at 14696 items/min)
2018-05-24 07:43:26 [scrapy.extensions.logstats] INFO: Crawled 2829544 pages (at 6765 pages/min), scraped 3298337 items (at 25049 items/min)
2018-05-24 07:44:26 [scrapy.extensions.logstats] INFO: Crawled 2835654 pages (at 6110 pages/min), scraped 3315739 items (at 17402 items/min)
2018-05-24 07:45:26 [scrapy.extensions.logstats] INFO: Crawled 2842212 pages (at 6558 pages/min), scraped 3317023 items (at 1284 items/min)
2018-05-24 07:46:26 [scrapy.extensions.logstats] INFO: Crawled 2849616 pages (at 7404 pages/min), scraped 3323335 items (at 6312 items/min)

2018-05-24 08:10:25 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: jd_spider)
2018-05-24 08:10:25 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (v3.5.2:4def2a2901a5, Jun 25 2016, 22:18:55) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 17.5.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-10-10.0.16299-SP0
2018-05-24 08:10:25 [scrapy.crawler] INFO: Overridden settings: {'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'NEWSPIDER_MODULE': 'jd_spider.spiders', 'SPIDER_MODULES': ['jd_spider.spiders'], 'LOG_LEVEL': 'INFO', 'LOG_FILE': 'JdSpider.log', 'BOT_NAME': 'jd_spider', 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'SPIDER_LOADER_WARN_ONLY': True, 'DOWNLOAD_TIMEOUT': 30}
2018-05-24 08:10:25 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.corestats.CoreStats']
2018-05-24 08:10:25 [jd] INFO: Reading start URLs from redis key 'JdSpider:start_urls' (batch size: 16, encoding: utf-8
2018-05-24 08:10:25 [scrapy.middleware] INFO: Enabled downloader middlewares:
['jd_spider.middlewares.JdSpiderDownloaderMiddleware',
 'jd_spider.middlewares.RandomProxy',
 'jd_spider.middlewares.MyRetryMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-05-24 08:10:25 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-05-24 08:10:25 [scrapy.middleware] INFO: Enabled item pipelines:
['scrapy_redis.pipelines.RedisPipeline']
2018-05-24 08:10:25 [scrapy.core.engine] INFO: Spider opened
2018-05-24 08:10:25 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-05-24 08:10:25 [jd] INFO: Spider opened: jd
2018-05-24 08:11:25 [scrapy.extensions.logstats] INFO: Crawled 4847 pages (at 4847 pages/min), scraped 10171 items (at 10171 items/min)
2018-05-24 08:12:25 [scrapy.extensions.logstats] INFO: Crawled 10529 pages (at 5682 pages/min), scraped 19926 items (at 9755 items/min)
2018-05-24 08:13:25 [scrapy.extensions.logstats] INFO: Crawled 14759 pages (at 4230 pages/min), scraped 21483 items (at 1557 items/min)
2018-05-24 08:14:25 [scrapy.extensions.logstats] INFO: Crawled 21120 pages (at 6361 pages/min), scraped 27507 items (at 6024 items/min)
2018-05-24 08:15:25 [scrapy.extensions.logstats] INFO: Crawled 26706 pages (at 5586 pages/min), scraped 29275 items (at 1768 items/min)
2018-05-24 08:16:25 [scrapy.extensions.logstats] INFO: Crawled 32722 pages (at 6016 pages/min), scraped 32107 items (at 2832 items/min)
2018-05-24 08:17:25 [scrapy.extensions.logstats] INFO: Crawled 38574 pages (at 5852 pages/min), scraped 32936 items (at 829 items/min)
2018-05-24 08:18:25 [scrapy.extensions.logstats] INFO: Crawled 44411 pages (at 5837 pages/min), scraped 33777 items (at 841 items/min)
2018-05-24 08:19:25 [scrapy.extensions.logstats] INFO: Crawled 50084 pages (at 5673 pages/min), scraped 34626 items (at 849 items/min)
2018-05-24 08:20:25 [scrapy.extensions.logstats] INFO: Crawled 55784 pages (at 5700 pages/min), scraped 37942 items (at 3316 items/min)
2018-05-24 08:21:25 [scrapy.extensions.logstats] INFO: Crawled 61946 pages (at 6162 pages/min), scraped 42748 items (at 4806 items/min)
2018-05-24 08:22:25 [scrapy.extensions.logstats] INFO: Crawled 67983 pages (at 6037 pages/min), scraped 44736 items (at 1988 items/min)
2018-05-24 08:23:25 [scrapy.extensions.logstats] INFO: Crawled 74127 pages (at 6144 pages/min), scraped 51917 items (at 7181 items/min)
2018-05-24 08:24:25 [scrapy.extensions.logstats] INFO: Crawled 79231 pages (at 5104 pages/min), scraped 74629 items (at 22712 items/min)
2018-05-24 08:25:25 [scrapy.extensions.logstats] INFO: Crawled 83878 pages (at 4647 pages/min), scraped 86017 items (at 11388 items/min)
2018-05-24 08:26:25 [scrapy.extensions.logstats] INFO: Crawled 89396 pages (at 5518 pages/min), scraped 87048 items (at 1031 items/min)
2018-05-24 08:27:25 [scrapy.extensions.logstats] INFO: Crawled 94957 pages (at 5561 pages/min), scraped 87822 items (at 774 items/min)
2018-05-24 08:28:25 [scrapy.extensions.logstats] INFO: Crawled 99944 pages (at 4987 pages/min), scraped 89600 items (at 1778 items/min)
2018-05-24 08:29:25 [scrapy.extensions.logstats] INFO: Crawled 105475 pages (at 5531 pages/min), scraped 93200 items (at 3600 items/min)
2018-05-24 08:30:25 [scrapy.extensions.logstats] INFO: Crawled 111529 pages (at 6054 pages/min), scraped 97386 items (at 4186 items/min)
2018-05-24 08:31:25 [scrapy.extensions.logstats] INFO: Crawled 117198 pages (at 5669 pages/min), scraped 104843 items (at 7457 items/min)
2018-05-24 08:32:25 [scrapy.extensions.logstats] INFO: Crawled 123485 pages (at 6287 pages/min), scraped 110562 items (at 5719 items/min)
2018-05-24 08:33:25 [scrapy.extensions.logstats] INFO: Crawled 129461 pages (at 5976 pages/min), scraped 111797 items (at 1235 items/min)
2018-05-24 08:34:25 [scrapy.extensions.logstats] INFO: Crawled 134730 pages (at 5269 pages/min), scraped 114692 items (at 2895 items/min)
2018-05-24 08:35:25 [scrapy.extensions.logstats] INFO: Crawled 140550 pages (at 5820 pages/min), scraped 116007 items (at 1315 items/min)
2018-05-24 08:36:25 [scrapy.extensions.logstats] INFO: Crawled 146344 pages (at 5794 pages/min), scraped 116960 items (at 953 items/min)
2018-05-24 08:37:25 [scrapy.extensions.logstats] INFO: Crawled 152125 pages (at 5781 pages/min), scraped 118206 items (at 1246 items/min)
2018-05-24 08:38:25 [scrapy.extensions.logstats] INFO: Crawled 157736 pages (at 5611 pages/min), scraped 119241 items (at 1035 items/min)
2018-05-24 08:39:25 [scrapy.extensions.logstats] INFO: Crawled 163637 pages (at 5901 pages/min), scraped 120247 items (at 1006 items/min)
2018-05-24 08:40:25 [scrapy.extensions.logstats] INFO: Crawled 169475 pages (at 5838 pages/min), scraped 121618 items (at 1371 items/min)
2018-05-24 08:41:25 [scrapy.extensions.logstats] INFO: Crawled 175257 pages (at 5782 pages/min), scraped 123459 items (at 1841 items/min)
2018-05-24 08:42:25 [scrapy.extensions.logstats] INFO: Crawled 181121 pages (at 5864 pages/min), scraped 125031 items (at 1572 items/min)
2018-05-24 08:43:25 [scrapy.extensions.logstats] INFO: Crawled 187255 pages (at 6134 pages/min), scraped 129856 items (at 4825 items/min)
2018-05-24 08:44:25 [scrapy.extensions.logstats] INFO: Crawled 193129 pages (at 5874 pages/min), scraped 143131 items (at 13275 items/min)
2018-05-24 08:45:25 [scrapy.extensions.logstats] INFO: Crawled 198684 pages (at 5555 pages/min), scraped 146989 items (at 3858 items/min)
2018-05-24 08:46:25 [scrapy.extensions.logstats] INFO: Crawled 204071 pages (at 5387 pages/min), scraped 147946 items (at 957 items/min)
2018-05-24 08:47:25 [scrapy.extensions.logstats] INFO: Crawled 209939 pages (at 5868 pages/min), scraped 148855 items (at 909 items/min)
2018-05-24 08:48:25 [scrapy.extensions.logstats] INFO: Crawled 215301 pages (at 5362 pages/min), scraped 149708 items (at 853 items/min)
2018-05-24 08:49:25 [scrapy.extensions.logstats] INFO: Crawled 221270 pages (at 5969 pages/min), scraped 152266 items (at 2558 items/min)
2018-05-24 08:50:25 [scrapy.extensions.logstats] INFO: Crawled 227129 pages (at 5859 pages/min), scraped 155533 items (at 3267 items/min)
2018-05-24 08:51:25 [scrapy.extensions.logstats] INFO: Crawled 233287 pages (at 6158 pages/min), scraped 162183 items (at 6650 items/min)
2018-05-24 08:52:25 [scrapy.extensions.logstats] INFO: Crawled 239135 pages (at 5848 pages/min), scraped 172031 items (at 9848 items/min)
2018-05-24 08:53:25 [scrapy.extensions.logstats] INFO: Crawled 244939 pages (at 5804 pages/min), scraped 173058 items (at 1027 items/min)
2018-05-24 08:54:25 [scrapy.extensions.logstats] INFO: Crawled 250853 pages (at 5914 pages/min), scraped 174240 items (at 1182 items/min)
2018-05-24 08:55:25 [scrapy.extensions.logstats] INFO: Crawled 256324 pages (at 5471 pages/min), scraped 176988 items (at 2748 items/min)
2018-05-24 08:56:25 [scrapy.extensions.logstats] INFO: Crawled 261918 pages (at 5594 pages/min), scraped 180902 items (at 3914 items/min)
2018-05-24 08:57:25 [scrapy.extensions.logstats] INFO: Crawled 267341 pages (at 5423 pages/min), scraped 185572 items (at 4670 items/min)
2018-05-24 08:58:25 [scrapy.extensions.logstats] INFO: Crawled 272822 pages (at 5481 pages/min), scraped 193966 items (at 8394 items/min)
2018-05-24 08:59:25 [scrapy.extensions.logstats] INFO: Crawled 278317 pages (at 5495 pages/min), scraped 206211 items (at 12245 items/min)
2018-05-24 09:00:25 [scrapy.extensions.logstats] INFO: Crawled 283345 pages (at 5028 pages/min), scraped 207443 items (at 1232 items/min)
2018-05-24 09:01:25 [scrapy.extensions.logstats] INFO: Crawled 288605 pages (at 5260 pages/min), scraped 211541 items (at 4098 items/min)
2018-05-24 09:02:25 [scrapy.extensions.logstats] INFO: Crawled 293959 pages (at 5354 pages/min), scraped 220410 items (at 8869 items/min)
2018-05-24 09:03:25 [scrapy.extensions.logstats] INFO: Crawled 299500 pages (at 5541 pages/min), scraped 230886 items (at 10476 items/min)
2018-05-24 09:04:25 [scrapy.extensions.logstats] INFO: Crawled 303734 pages (at 4234 pages/min), scraped 239895 items (at 9009 items/min)
2018-05-24 09:05:25 [scrapy.extensions.logstats] INFO: Crawled 308247 pages (at 4513 pages/min), scraped 249693 items (at 9798 items/min)
2018-05-24 09:06:25 [scrapy.extensions.logstats] INFO: Crawled 312670 pages (at 4423 pages/min), scraped 252876 items (at 3183 items/min)
2018-05-24 09:07:25 [scrapy.extensions.logstats] INFO: Crawled 317302 pages (at 4632 pages/min), scraped 261385 items (at 8509 items/min)
2018-05-24 09:08:25 [scrapy.extensions.logstats] INFO: Crawled 321704 pages (at 4402 pages/min), scraped 267048 items (at 5663 items/min)
2018-05-24 09:09:25 [scrapy.extensions.logstats] INFO: Crawled 326729 pages (at 5025 pages/min), scraped 272618 items (at 5570 items/min)
2018-05-24 09:10:25 [scrapy.extensions.logstats] INFO: Crawled 330271 pages (at 3542 pages/min), scraped 279747 items (at 7129 items/min)
2018-05-24 09:11:25 [scrapy.extensions.logstats] INFO: Crawled 334767 pages (at 4496 pages/min), scraped 295140 items (at 15393 items/min)
2018-05-24 09:12:25 [scrapy.extensions.logstats] INFO: Crawled 338659 pages (at 3892 pages/min), scraped 305202 items (at 10062 items/min)
2018-05-24 09:13:25 [scrapy.extensions.logstats] INFO: Crawled 342550 pages (at 3891 pages/min), scraped 307954 items (at 2752 items/min)
2018-05-24 09:14:25 [scrapy.extensions.logstats] INFO: Crawled 345625 pages (at 3075 pages/min), scraped 308393 items (at 439 items/min)
2018-05-24 09:15:25 [scrapy.extensions.logstats] INFO: Crawled 349258 pages (at 3633 pages/min), scraped 309336 items (at 943 items/min)
2018-05-24 09:16:25 [scrapy.extensions.logstats] INFO: Crawled 349974 pages (at 716 pages/min), scraped 309674 items (at 338 items/min)
2018-05-24 09:17:25 [scrapy.extensions.logstats] INFO: Crawled 350920 pages (at 946 pages/min), scraped 311585 items (at 1911 items/min)
2018-05-24 09:18:25 [scrapy.extensions.logstats] INFO: Crawled 351793 pages (at 873 pages/min), scraped 314108 items (at 2523 items/min)
2018-05-24 09:19:25 [scrapy.extensions.logstats] INFO: Crawled 357091 pages (at 5298 pages/min), scraped 326269 items (at 12161 items/min)
2018-05-24 09:20:25 [scrapy.extensions.logstats] INFO: Crawled 362533 pages (at 5442 pages/min), scraped 343572 items (at 17303 items/min)
2018-05-24 09:21:25 [scrapy.extensions.logstats] INFO: Crawled 366961 pages (at 4428 pages/min), scraped 360603 items (at 17031 items/min)
2018-05-24 09:22:25 [scrapy.extensions.logstats] INFO: Crawled 371452 pages (at 4491 pages/min), scraped 379727 items (at 19124 items/min)
2018-05-24 09:23:25 [scrapy.extensions.logstats] INFO: Crawled 375892 pages (at 4440 pages/min), scraped 399554 items (at 19827 items/min)
2018-05-24 09:24:25 [scrapy.extensions.logstats] INFO: Crawled 380163 pages (at 4271 pages/min), scraped 421110 items (at 21556 items/min)
2018-05-24 09:25:25 [scrapy.extensions.logstats] INFO: Crawled 384712 pages (at 4549 pages/min), scraped 435139 items (at 14029 items/min)
2018-05-24 09:26:25 [scrapy.extensions.logstats] INFO: Crawled 389129 pages (at 4417 pages/min), scraped 448943 items (at 13804 items/min)
2018-05-24 09:27:25 [scrapy.extensions.logstats] INFO: Crawled 393663 pages (at 4534 pages/min), scraped 467426 items (at 18483 items/min)
2018-05-24 09:28:25 [scrapy.extensions.logstats] INFO: Crawled 398377 pages (at 4714 pages/min), scraped 482595 items (at 15169 items/min)
2018-05-24 09:29:25 [scrapy.extensions.logstats] INFO: Crawled 403405 pages (at 5028 pages/min), scraped 501534 items (at 18939 items/min)
2018-05-24 09:30:25 [scrapy.extensions.logstats] INFO: Crawled 408511 pages (at 5106 pages/min), scraped 520402 items (at 18868 items/min)
2018-05-24 09:31:25 [scrapy.extensions.logstats] INFO: Crawled 412990 pages (at 4479 pages/min), scraped 534720 items (at 14318 items/min)
2018-05-24 09:32:25 [scrapy.extensions.logstats] INFO: Crawled 417798 pages (at 4808 pages/min), scraped 548738 items (at 14018 items/min)
2018-05-24 09:33:25 [scrapy.extensions.logstats] INFO: Crawled 422348 pages (at 4550 pages/min), scraped 569120 items (at 20382 items/min)
2018-05-24 09:34:25 [scrapy.extensions.logstats] INFO: Crawled 426695 pages (at 4347 pages/min), scraped 588586 items (at 19466 items/min)
2018-05-24 09:35:25 [scrapy.extensions.logstats] INFO: Crawled 430010 pages (at 3315 pages/min), scraped 600253 items (at 11667 items/min)
2018-05-24 09:36:25 [scrapy.extensions.logstats] INFO: Crawled 434467 pages (at 4457 pages/min), scraped 617195 items (at 16942 items/min)
2018-05-24 09:37:25 [scrapy.extensions.logstats] INFO: Crawled 439496 pages (at 5029 pages/min), scraped 632107 items (at 14912 items/min)
2018-05-24 09:38:25 [scrapy.extensions.logstats] INFO: Crawled 443970 pages (at 4474 pages/min), scraped 644043 items (at 11936 items/min)
2018-05-24 09:39:25 [scrapy.extensions.logstats] INFO: Crawled 448255 pages (at 4285 pages/min), scraped 653237 items (at 9194 items/min)
2018-05-24 09:40:25 [scrapy.extensions.logstats] INFO: Crawled 453251 pages (at 4996 pages/min), scraped 659769 items (at 6532 items/min)
2018-05-24 09:41:25 [scrapy.extensions.logstats] INFO: Crawled 458882 pages (at 5631 pages/min), scraped 668202 items (at 8433 items/min)
2018-05-24 09:42:25 [scrapy.extensions.logstats] INFO: Crawled 463609 pages (at 4727 pages/min), scraped 674977 items (at 6775 items/min)
2018-05-24 09:43:25 [scrapy.extensions.logstats] INFO: Crawled 468266 pages (at 4657 pages/min), scraped 688035 items (at 13058 items/min)
2018-05-24 09:44:25 [scrapy.extensions.logstats] INFO: Crawled 472827 pages (at 4561 pages/min), scraped 698477 items (at 10442 items/min)
2018-05-24 09:45:25 [scrapy.extensions.logstats] INFO: Crawled 477325 pages (at 4498 pages/min), scraped 716647 items (at 18170 items/min)
2018-05-24 09:46:25 [scrapy.extensions.logstats] INFO: Crawled 480914 pages (at 3589 pages/min), scraped 731015 items (at 14368 items/min)
2018-05-24 09:47:25 [scrapy.extensions.logstats] INFO: Crawled 485937 pages (at 5023 pages/min), scraped 747341 items (at 16326 items/min)
2018-05-24 09:48:25 [scrapy.extensions.logstats] INFO: Crawled 491084 pages (at 5147 pages/min), scraped 765475 items (at 18134 items/min)
2018-05-24 09:49:25 [scrapy.extensions.logstats] INFO: Crawled 495792 pages (at 4708 pages/min), scraped 777146 items (at 11671 items/min)
2018-05-24 09:50:25 [scrapy.extensions.logstats] INFO: Crawled 500904 pages (at 5112 pages/min), scraped 793523 items (at 16377 items/min)
2018-05-24 09:51:25 [scrapy.extensions.logstats] INFO: Crawled 506299 pages (at 5395 pages/min), scraped 803934 items (at 10411 items/min)
2018-05-24 09:52:25 [scrapy.extensions.logstats] INFO: Crawled 512021 pages (at 5722 pages/min), scraped 810554 items (at 6620 items/min)
2018-05-24 09:53:25 [scrapy.extensions.logstats] INFO: Crawled 517775 pages (at 5754 pages/min), scraped 824543 items (at 13989 items/min)
2018-05-24 09:54:25 [scrapy.extensions.logstats] INFO: Crawled 523381 pages (at 5606 pages/min), scraped 831494 items (at 6951 items/min)
2018-05-24 09:55:25 [scrapy.extensions.logstats] INFO: Crawled 528436 pages (at 5055 pages/min), scraped 851965 items (at 20471 items/min)
2018-05-24 09:56:25 [scrapy.extensions.logstats] INFO: Crawled 533838 pages (at 5402 pages/min), scraped 866235 items (at 14270 items/min)
2018-05-24 09:57:25 [scrapy.extensions.logstats] INFO: Crawled 538965 pages (at 5127 pages/min), scraped 883962 items (at 17727 items/min)
2018-05-24 09:58:25 [scrapy.extensions.logstats] INFO: Crawled 543569 pages (at 4604 pages/min), scraped 895444 items (at 11482 items/min)
2018-05-24 09:59:25 [scrapy.extensions.logstats] INFO: Crawled 549066 pages (at 5497 pages/min), scraped 910399 items (at 14955 items/min)
2018-05-24 10:00:25 [scrapy.extensions.logstats] INFO: Crawled 554408 pages (at 5342 pages/min), scraped 921866 items (at 11467 items/min)
2018-05-24 10:01:25 [scrapy.extensions.logstats] INFO: Crawled 559992 pages (at 5584 pages/min), scraped 936433 items (at 14567 items/min)
2018-05-24 10:02:25 [scrapy.extensions.logstats] INFO: Crawled 565883 pages (at 5891 pages/min), scraped 960621 items (at 24188 items/min)
2018-05-24 10:03:25 [scrapy.extensions.logstats] INFO: Crawled 571107 pages (at 5224 pages/min), scraped 988842 items (at 28221 items/min)
2018-05-24 10:04:25 [scrapy.extensions.logstats] INFO: Crawled 577199 pages (at 6092 pages/min), scraped 1012247 items (at 23405 items/min)
2018-05-24 10:05:25 [scrapy.extensions.logstats] INFO: Crawled 582508 pages (at 5309 pages/min), scraped 1032666 items (at 20419 items/min)
2018-05-24 10:06:25 [scrapy.extensions.logstats] INFO: Crawled 587446 pages (at 4938 pages/min), scraped 1046549 items (at 13883 items/min)
2018-05-24 10:07:25 [scrapy.extensions.logstats] INFO: Crawled 593267 pages (at 5821 pages/min), scraped 1065006 items (at 18457 items/min)
2018-05-24 10:08:25 [scrapy.extensions.logstats] INFO: Crawled 598857 pages (at 5590 pages/min), scraped 1073296 items (at 8290 items/min)
2018-05-24 10:09:25 [scrapy.extensions.logstats] INFO: Crawled 604732 pages (at 5875 pages/min), scraped 1082156 items (at 8860 items/min)
2018-05-24 10:10:25 [scrapy.extensions.logstats] INFO: Crawled 609388 pages (at 4656 pages/min), scraped 1095094 items (at 12938 items/min)
2018-05-24 10:11:25 [scrapy.extensions.logstats] INFO: Crawled 614736 pages (at 5348 pages/min), scraped 1114072 items (at 18978 items/min)
2018-05-24 10:12:25 [scrapy.extensions.logstats] INFO: Crawled 619673 pages (at 4937 pages/min), scraped 1133840 items (at 19768 items/min)
2018-05-24 10:13:25 [scrapy.extensions.logstats] INFO: Crawled 624189 pages (at 4516 pages/min), scraped 1158908 items (at 25068 items/min)
2018-05-24 10:14:25 [scrapy.extensions.logstats] INFO: Crawled 629370 pages (at 5181 pages/min), scraped 1179820 items (at 20912 items/min)
2018-05-24 10:15:25 [scrapy.extensions.logstats] INFO: Crawled 635022 pages (at 5652 pages/min), scraped 1196914 items (at 17094 items/min)
2018-05-24 10:16:25 [scrapy.extensions.logstats] INFO: Crawled 640604 pages (at 5582 pages/min), scraped 1214788 items (at 17874 items/min)
2018-05-24 10:17:25 [scrapy.extensions.logstats] INFO: Crawled 646332 pages (at 5728 pages/min), scraped 1229899 items (at 15111 items/min)
2018-05-24 10:18:25 [scrapy.extensions.logstats] INFO: Crawled 652009 pages (at 5677 pages/min), scraped 1244103 items (at 14204 items/min)
2018-05-24 10:19:25 [scrapy.extensions.logstats] INFO: Crawled 658595 pages (at 6586 pages/min), scraped 1245352 items (at 1249 items/min)
2018-05-24 10:20:25 [scrapy.extensions.logstats] INFO: Crawled 664922 pages (at 6327 pages/min), scraped 1246551 items (at 1199 items/min)
2018-05-24 10:21:25 [scrapy.extensions.logstats] INFO: Crawled 672328 pages (at 7406 pages/min), scraped 1248182 items (at 1631 items/min)
2018-05-24 10:22:25 [scrapy.extensions.logstats] INFO: Crawled 680070 pages (at 7742 pages/min), scraped 1249890 items (at 1708 items/min)
2018-05-24 10:23:25 [scrapy.extensions.logstats] INFO: Crawled 687590 pages (at 7520 pages/min), scraped 1252570 items (at 2680 items/min)
2018-05-24 10:24:25 [scrapy.extensions.logstats] INFO: Crawled 694356 pages (at 6766 pages/min), scraped 1253879 items (at 1309 items/min)
2018-05-24 10:25:25 [scrapy.extensions.logstats] INFO: Crawled 700756 pages (at 6400 pages/min), scraped 1254987 items (at 1108 items/min)
2018-05-24 10:26:25 [scrapy.extensions.logstats] INFO: Crawled 707158 pages (at 6402 pages/min), scraped 1255931 items (at 944 items/min)
2018-05-24 10:27:25 [scrapy.extensions.logstats] INFO: Crawled 713793 pages (at 6635 pages/min), scraped 1257336 items (at 1405 items/min)
2018-05-24 10:28:25 [scrapy.extensions.logstats] INFO: Crawled 720472 pages (at 6679 pages/min), scraped 1259652 items (at 2316 items/min)
2018-05-24 10:29:25 [scrapy.extensions.logstats] INFO: Crawled 726771 pages (at 6299 pages/min), scraped 1261082 items (at 1430 items/min)
2018-05-24 10:30:25 [scrapy.extensions.logstats] INFO: Crawled 732726 pages (at 5955 pages/min), scraped 1262221 items (at 1139 items/min)
2018-05-24 10:31:25 [scrapy.extensions.logstats] INFO: Crawled 738663 pages (at 5937 pages/min), scraped 1263265 items (at 1044 items/min)
2018-05-24 10:32:25 [scrapy.extensions.logstats] INFO: Crawled 744971 pages (at 6308 pages/min), scraped 1264587 items (at 1322 items/min)
2018-05-24 10:33:25 [scrapy.extensions.logstats] INFO: Crawled 750521 pages (at 5550 pages/min), scraped 1278869 items (at 14282 items/min)
2018-05-24 10:34:25 [scrapy.extensions.logstats] INFO: Crawled 755845 pages (at 5324 pages/min), scraped 1290844 items (at 11975 items/min)
2018-05-24 10:35:25 [scrapy.extensions.logstats] INFO: Crawled 760876 pages (at 5031 pages/min), scraped 1302893 items (at 12049 items/min)
2018-05-24 10:36:25 [scrapy.extensions.logstats] INFO: Crawled 766095 pages (at 5219 pages/min), scraped 1313943 items (at 11050 items/min)
2018-05-24 10:37:25 [scrapy.extensions.logstats] INFO: Crawled 771915 pages (at 5820 pages/min), scraped 1319944 items (at 6001 items/min)
2018-05-24 10:38:25 [scrapy.extensions.logstats] INFO: Crawled 778366 pages (at 6451 pages/min), scraped 1328232 items (at 8288 items/min)

2018-05-24 20:29:10 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: jd_spider)
2018-05-24 20:29:10 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (v3.5.2:4def2a2901a5, Jun 25 2016, 22:18:55) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 17.5.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-10-10.0.16299-SP0
2018-05-24 20:29:10 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'jd_spider', 'LOG_FILE': 'JdSpider.log', 'SPIDER_LOADER_WARN_ONLY': True, 'NEWSPIDER_MODULE': 'jd_spider.spiders', 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'DOWNLOAD_TIMEOUT': 30, 'LOG_LEVEL': 'INFO', 'SPIDER_MODULES': ['jd_spider.spiders'], 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler'}
2018-05-24 20:29:10 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.logstats.LogStats']
2018-05-24 20:29:10 [jd] INFO: Reading start URLs from redis key 'JdSpider:start_urls' (batch size: 16, encoding: utf-8
2018-05-24 20:29:11 [twisted] CRITICAL: Unhandled error in Deferred:
2018-05-24 20:29:11 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connection.py", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\util\connection.py", line 83, in create_connection
    raise err
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connectionpool.py", line 357, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "D:\Python35\Lib\http\client.py", line 1106, in request
    self._send_request(method, url, body, headers)
  File "D:\Python35\Lib\http\client.py", line 1151, in _send_request
    self.endheaders(body)
  File "D:\Python35\Lib\http\client.py", line 1102, in endheaders
    self._send_output(message_body)
  File "D:\Python35\Lib\http\client.py", line 934, in _send_output
    self.send(msg)
  File "D:\Python35\Lib\http\client.py", line 877, in send
    self.connect()
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connection.py", line 166, in connect
    conn = self._new_conn()
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connection.py", line 150, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000024B336EC828>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=8000): Max retries exceeded with url: /?count=40 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000024B336EC828>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。',))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\crawler.py", line 80, in crawl
    self.engine = self._create_engine()
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\crawler.py", line 105, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\engine.py", line 69, in __init__
    self.downloader = downloader_cls(crawler)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\__init__.py", line 88, in __init__
    self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\middleware.py", line 58, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\middleware.py", line 34, in from_settings
    mwcls = load_object(clspath)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\utils\misc.py", line 44, in load_object
    mod = import_module(module)
  File "C:\Users\allen\Envs\scrapy_py3\lib\importlib\__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 986, in _gcd_import
  File "<frozen importlib._bootstrap>", line 969, in _find_and_load
  File "<frozen importlib._bootstrap>", line 958, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 673, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 665, in exec_module
  File "<frozen importlib._bootstrap>", line 222, in _call_with_frames_removed
  File "E:\python-learning\scrapy\jd_spider\jd_spider\jd_spider\jd_spider\middlewares.py", line 24, in <module>
    IP_PORT_LIST = get_ip_port_list()
  File "E:\python-learning\scrapy\jd_spider\jd_spider\jd_spider\jd_spider\get_proxy.py", line 28, in get_ip_port_list
    r = requests.get('http://127.0.0.1:8000/?count=40')
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\adapters.py", line 508, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='127.0.0.1', port=8000): Max retries exceeded with url: /?count=40 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000024B336EC828>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。',))
2018-05-24 20:29:28 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: jd_spider)
2018-05-24 20:29:28 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (v3.5.2:4def2a2901a5, Jun 25 2016, 22:18:55) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 17.5.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-10-10.0.16299-SP0
2018-05-24 20:29:28 [scrapy.crawler] INFO: Overridden settings: {'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'SPIDER_LOADER_WARN_ONLY': True, 'LOG_FILE': 'JdSpider.log', 'BOT_NAME': 'jd_spider', 'LOG_LEVEL': 'INFO', 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'DOWNLOAD_TIMEOUT': 30, 'SPIDER_MODULES': ['jd_spider.spiders'], 'NEWSPIDER_MODULE': 'jd_spider.spiders'}
2018-05-24 20:29:28 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole']
2018-05-24 20:29:28 [jd] INFO: Reading start URLs from redis key 'JdSpider:start_urls' (batch size: 16, encoding: utf-8
2018-05-24 20:29:29 [twisted] CRITICAL: Unhandled error in Deferred:
2018-05-24 20:29:29 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connection.py", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\util\connection.py", line 83, in create_connection
    raise err
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connectionpool.py", line 357, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "D:\Python35\Lib\http\client.py", line 1106, in request
    self._send_request(method, url, body, headers)
  File "D:\Python35\Lib\http\client.py", line 1151, in _send_request
    self.endheaders(body)
  File "D:\Python35\Lib\http\client.py", line 1102, in endheaders
    self._send_output(message_body)
  File "D:\Python35\Lib\http\client.py", line 934, in _send_output
    self.send(msg)
  File "D:\Python35\Lib\http\client.py", line 877, in send
    self.connect()
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connection.py", line 166, in connect
    conn = self._new_conn()
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connection.py", line 150, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000002371BD9C828>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=8000): Max retries exceeded with url: /?count=40 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002371BD9C828>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。',))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\crawler.py", line 80, in crawl
    self.engine = self._create_engine()
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\crawler.py", line 105, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\engine.py", line 69, in __init__
    self.downloader = downloader_cls(crawler)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\__init__.py", line 88, in __init__
    self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\middleware.py", line 58, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\middleware.py", line 34, in from_settings
    mwcls = load_object(clspath)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\utils\misc.py", line 44, in load_object
    mod = import_module(module)
  File "C:\Users\allen\Envs\scrapy_py3\lib\importlib\__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 986, in _gcd_import
  File "<frozen importlib._bootstrap>", line 969, in _find_and_load
  File "<frozen importlib._bootstrap>", line 958, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 673, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 665, in exec_module
  File "<frozen importlib._bootstrap>", line 222, in _call_with_frames_removed
  File "E:\python-learning\scrapy\jd_spider\jd_spider\jd_spider\jd_spider\middlewares.py", line 24, in <module>
    IP_PORT_LIST = get_ip_port_list()
  File "E:\python-learning\scrapy\jd_spider\jd_spider\jd_spider\jd_spider\get_proxy.py", line 28, in get_ip_port_list
    r = requests.get('http://127.0.0.1:8000/?count=40')
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\adapters.py", line 508, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='127.0.0.1', port=8000): Max retries exceeded with url: /?count=40 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002371BD9C828>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。',))
2018-05-24 20:30:59 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: jd_spider)
2018-05-24 20:30:59 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (v3.5.2:4def2a2901a5, Jun 25 2016, 22:18:55) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 17.5.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-10-10.0.16299-SP0
2018-05-24 20:30:59 [scrapy.crawler] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'jd_spider.spiders', 'DOWNLOAD_TIMEOUT': 30, 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'LOG_FILE': 'JdSpider.log', 'SPIDER_MODULES': ['jd_spider.spiders'], 'LOG_LEVEL': 'INFO', 'SPIDER_LOADER_WARN_ONLY': True, 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'BOT_NAME': 'jd_spider'}
2018-05-24 20:30:59 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2018-05-24 20:30:59 [jd] INFO: Reading start URLs from redis key 'JdSpider:start_urls' (batch size: 16, encoding: utf-8
2018-05-24 20:31:00 [twisted] CRITICAL: Unhandled error in Deferred:
2018-05-24 20:31:00 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connection.py", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\util\connection.py", line 83, in create_connection
    raise err
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connectionpool.py", line 357, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "D:\Python35\Lib\http\client.py", line 1106, in request
    self._send_request(method, url, body, headers)
  File "D:\Python35\Lib\http\client.py", line 1151, in _send_request
    self.endheaders(body)
  File "D:\Python35\Lib\http\client.py", line 1102, in endheaders
    self._send_output(message_body)
  File "D:\Python35\Lib\http\client.py", line 934, in _send_output
    self.send(msg)
  File "D:\Python35\Lib\http\client.py", line 877, in send
    self.connect()
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connection.py", line 166, in connect
    conn = self._new_conn()
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connection.py", line 150, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001AECB28C828>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=8000): Max retries exceeded with url: /?count=40 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001AECB28C828>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。',))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\crawler.py", line 80, in crawl
    self.engine = self._create_engine()
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\crawler.py", line 105, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\engine.py", line 69, in __init__
    self.downloader = downloader_cls(crawler)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\__init__.py", line 88, in __init__
    self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\middleware.py", line 58, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\middleware.py", line 34, in from_settings
    mwcls = load_object(clspath)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\utils\misc.py", line 44, in load_object
    mod = import_module(module)
  File "C:\Users\allen\Envs\scrapy_py3\lib\importlib\__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 986, in _gcd_import
  File "<frozen importlib._bootstrap>", line 969, in _find_and_load
  File "<frozen importlib._bootstrap>", line 958, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 673, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 665, in exec_module
  File "<frozen importlib._bootstrap>", line 222, in _call_with_frames_removed
  File "E:\python-learning\scrapy\jd_spider\jd_spider\jd_spider\jd_spider\middlewares.py", line 24, in <module>
    IP_PORT_LIST = get_ip_port_list()
  File "E:\python-learning\scrapy\jd_spider\jd_spider\jd_spider\jd_spider\get_proxy.py", line 28, in get_ip_port_list
    r = requests.get('http://127.0.0.1:8000/?count=40')
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\adapters.py", line 508, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='127.0.0.1', port=8000): Max retries exceeded with url: /?count=40 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001AECB28C828>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。',))
2018-05-24 20:32:34 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: jd_spider)
2018-05-24 20:32:34 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (v3.5.2:4def2a2901a5, Jun 25 2016, 22:18:55) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 17.5.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-10-10.0.16299-SP0
2018-05-24 20:32:34 [scrapy.crawler] INFO: Overridden settings: {'LOG_LEVEL': 'INFO', 'BOT_NAME': 'jd_spider', 'NEWSPIDER_MODULE': 'jd_spider.spiders', 'DOWNLOAD_TIMEOUT': 30, 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'LOG_FILE': 'JdSpider.log', 'SPIDER_LOADER_WARN_ONLY': True, 'SPIDER_MODULES': ['jd_spider.spiders'], 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter'}
2018-05-24 20:32:34 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole']
2018-05-24 20:32:34 [jd] INFO: Reading start URLs from redis key 'JdSpider:start_urls' (batch size: 16, encoding: utf-8
2018-05-24 20:32:35 [twisted] CRITICAL: Unhandled error in Deferred:
2018-05-24 20:32:35 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connection.py", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\util\connection.py", line 83, in create_connection
    raise err
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connectionpool.py", line 357, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "D:\Python35\Lib\http\client.py", line 1106, in request
    self._send_request(method, url, body, headers)
  File "D:\Python35\Lib\http\client.py", line 1151, in _send_request
    self.endheaders(body)
  File "D:\Python35\Lib\http\client.py", line 1102, in endheaders
    self._send_output(message_body)
  File "D:\Python35\Lib\http\client.py", line 934, in _send_output
    self.send(msg)
  File "D:\Python35\Lib\http\client.py", line 877, in send
    self.connect()
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connection.py", line 166, in connect
    conn = self._new_conn()
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connection.py", line 150, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001A8E12CC828>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=8000): Max retries exceeded with url: /?count=40 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001A8E12CC828>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。',))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\crawler.py", line 80, in crawl
    self.engine = self._create_engine()
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\crawler.py", line 105, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\engine.py", line 69, in __init__
    self.downloader = downloader_cls(crawler)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\__init__.py", line 88, in __init__
    self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\middleware.py", line 58, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\middleware.py", line 34, in from_settings
    mwcls = load_object(clspath)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\utils\misc.py", line 44, in load_object
    mod = import_module(module)
  File "C:\Users\allen\Envs\scrapy_py3\lib\importlib\__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 986, in _gcd_import
  File "<frozen importlib._bootstrap>", line 969, in _find_and_load
  File "<frozen importlib._bootstrap>", line 958, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 673, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 665, in exec_module
  File "<frozen importlib._bootstrap>", line 222, in _call_with_frames_removed
  File "E:\python-learning\scrapy\jd_spider\jd_spider\jd_spider\jd_spider\middlewares.py", line 24, in <module>
    IP_PORT_LIST = get_ip_port_list()
  File "E:\python-learning\scrapy\jd_spider\jd_spider\jd_spider\jd_spider\get_proxy.py", line 28, in get_ip_port_list
    r = requests.get('http://127.0.0.1:8000/?count=40')
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\adapters.py", line 508, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='127.0.0.1', port=8000): Max retries exceeded with url: /?count=40 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001A8E12CC828>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。',))
2018-05-24 20:32:50 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: jd_spider)
2018-05-24 20:32:50 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (v3.5.2:4def2a2901a5, Jun 25 2016, 22:18:55) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 17.5.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-10-10.0.16299-SP0
2018-05-24 20:32:50 [scrapy.crawler] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'jd_spider.spiders', 'SPIDER_MODULES': ['jd_spider.spiders'], 'DOWNLOAD_TIMEOUT': 30, 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'LOG_LEVEL': 'INFO', 'SPIDER_LOADER_WARN_ONLY': True, 'BOT_NAME': 'jd_spider', 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'LOG_FILE': 'JdSpider.log'}
2018-05-24 20:32:50 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole']
2018-05-24 20:32:50 [jd] INFO: Reading start URLs from redis key 'JdSpider:start_urls' (batch size: 16, encoding: utf-8
2018-05-24 20:32:51 [twisted] CRITICAL: Unhandled error in Deferred:
2018-05-24 20:32:51 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connection.py", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\util\connection.py", line 83, in create_connection
    raise err
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connectionpool.py", line 357, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "D:\Python35\Lib\http\client.py", line 1106, in request
    self._send_request(method, url, body, headers)
  File "D:\Python35\Lib\http\client.py", line 1151, in _send_request
    self.endheaders(body)
  File "D:\Python35\Lib\http\client.py", line 1102, in endheaders
    self._send_output(message_body)
  File "D:\Python35\Lib\http\client.py", line 934, in _send_output
    self.send(msg)
  File "D:\Python35\Lib\http\client.py", line 877, in send
    self.connect()
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connection.py", line 166, in connect
    conn = self._new_conn()
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connection.py", line 150, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000022E032FC828>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=8000): Max retries exceeded with url: /?count=40 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000022E032FC828>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。',))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\crawler.py", line 80, in crawl
    self.engine = self._create_engine()
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\crawler.py", line 105, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\engine.py", line 69, in __init__
    self.downloader = downloader_cls(crawler)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\__init__.py", line 88, in __init__
    self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\middleware.py", line 58, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\middleware.py", line 34, in from_settings
    mwcls = load_object(clspath)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\utils\misc.py", line 44, in load_object
    mod = import_module(module)
  File "C:\Users\allen\Envs\scrapy_py3\lib\importlib\__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 986, in _gcd_import
  File "<frozen importlib._bootstrap>", line 969, in _find_and_load
  File "<frozen importlib._bootstrap>", line 958, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 673, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 665, in exec_module
  File "<frozen importlib._bootstrap>", line 222, in _call_with_frames_removed
  File "E:\python-learning\scrapy\jd_spider\jd_spider\jd_spider\jd_spider\middlewares.py", line 24, in <module>
    IP_PORT_LIST = get_ip_port_list()
  File "E:\python-learning\scrapy\jd_spider\jd_spider\jd_spider\jd_spider\get_proxy.py", line 28, in get_ip_port_list
    r = requests.get('http://127.0.0.1:8000/?count=40')
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\adapters.py", line 508, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='127.0.0.1', port=8000): Max retries exceeded with url: /?count=40 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000022E032FC828>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。',))
2018-05-24 20:33:05 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: jd_spider)
2018-05-24 20:33:05 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (v3.5.2:4def2a2901a5, Jun 25 2016, 22:18:55) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 17.5.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-10-10.0.16299-SP0
2018-05-24 20:33:05 [scrapy.crawler] INFO: Overridden settings: {'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'SPIDER_LOADER_WARN_ONLY': True, 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'jd_spider.spiders', 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'BOT_NAME': 'jd_spider', 'LOG_FILE': 'JdSpider.log', 'SPIDER_MODULES': ['jd_spider.spiders'], 'DOWNLOAD_TIMEOUT': 30}
2018-05-24 20:33:05 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole']
2018-05-24 20:33:05 [jd] INFO: Reading start URLs from redis key 'JdSpider:start_urls' (batch size: 16, encoding: utf-8
2018-05-24 20:33:07 [twisted] CRITICAL: Unhandled error in Deferred:
2018-05-24 20:33:07 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connection.py", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\util\connection.py", line 83, in create_connection
    raise err
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connectionpool.py", line 357, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "D:\Python35\Lib\http\client.py", line 1106, in request
    self._send_request(method, url, body, headers)
  File "D:\Python35\Lib\http\client.py", line 1151, in _send_request
    self.endheaders(body)
  File "D:\Python35\Lib\http\client.py", line 1102, in endheaders
    self._send_output(message_body)
  File "D:\Python35\Lib\http\client.py", line 934, in _send_output
    self.send(msg)
  File "D:\Python35\Lib\http\client.py", line 877, in send
    self.connect()
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connection.py", line 166, in connect
    conn = self._new_conn()
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connection.py", line 150, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000022B7D1EC828>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=8000): Max retries exceeded with url: /?count=40 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000022B7D1EC828>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。',))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\crawler.py", line 80, in crawl
    self.engine = self._create_engine()
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\crawler.py", line 105, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\engine.py", line 69, in __init__
    self.downloader = downloader_cls(crawler)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\__init__.py", line 88, in __init__
    self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\middleware.py", line 58, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\middleware.py", line 34, in from_settings
    mwcls = load_object(clspath)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\utils\misc.py", line 44, in load_object
    mod = import_module(module)
  File "C:\Users\allen\Envs\scrapy_py3\lib\importlib\__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 986, in _gcd_import
  File "<frozen importlib._bootstrap>", line 969, in _find_and_load
  File "<frozen importlib._bootstrap>", line 958, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 673, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 665, in exec_module
  File "<frozen importlib._bootstrap>", line 222, in _call_with_frames_removed
  File "E:\python-learning\scrapy\jd_spider\jd_spider\jd_spider\jd_spider\middlewares.py", line 24, in <module>
    IP_PORT_LIST = get_ip_port_list()
  File "E:\python-learning\scrapy\jd_spider\jd_spider\jd_spider\jd_spider\get_proxy.py", line 28, in get_ip_port_list
    r = requests.get('http://127.0.0.1:8000/?count=40')
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\adapters.py", line 508, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='127.0.0.1', port=8000): Max retries exceeded with url: /?count=40 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000022B7D1EC828>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。',))
2018-05-24 20:34:24 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: jd_spider)
2018-05-24 20:34:24 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (v3.5.2:4def2a2901a5, Jun 25 2016, 22:18:55) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 17.5.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-10-10.0.16299-SP0
2018-05-24 20:34:24 [scrapy.crawler] INFO: Overridden settings: {'LOG_FILE': 'JdSpider.log', 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'SPIDER_MODULES': ['jd_spider.spiders'], 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'BOT_NAME': 'jd_spider', 'DOWNLOAD_TIMEOUT': 30, 'NEWSPIDER_MODULE': 'jd_spider.spiders', 'LOG_LEVEL': 'INFO', 'SPIDER_LOADER_WARN_ONLY': True}
2018-05-24 20:34:24 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2018-05-24 20:34:24 [jd] INFO: Reading start URLs from redis key 'JdSpider:start_urls' (batch size: 16, encoding: utf-8
2018-05-24 20:34:25 [twisted] CRITICAL: Unhandled error in Deferred:
2018-05-24 20:34:25 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connection.py", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\util\connection.py", line 83, in create_connection
    raise err
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connectionpool.py", line 357, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "D:\Python35\Lib\http\client.py", line 1106, in request
    self._send_request(method, url, body, headers)
  File "D:\Python35\Lib\http\client.py", line 1151, in _send_request
    self.endheaders(body)
  File "D:\Python35\Lib\http\client.py", line 1102, in endheaders
    self._send_output(message_body)
  File "D:\Python35\Lib\http\client.py", line 934, in _send_output
    self.send(msg)
  File "D:\Python35\Lib\http\client.py", line 877, in send
    self.connect()
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connection.py", line 166, in connect
    conn = self._new_conn()
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connection.py", line 150, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000022B08BC9B70>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=8000): Max retries exceeded with url: /?count=40 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000022B08BC9B70>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。',))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\crawler.py", line 80, in crawl
    self.engine = self._create_engine()
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\crawler.py", line 105, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\engine.py", line 69, in __init__
    self.downloader = downloader_cls(crawler)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\__init__.py", line 88, in __init__
    self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\middleware.py", line 58, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\middleware.py", line 34, in from_settings
    mwcls = load_object(clspath)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\utils\misc.py", line 44, in load_object
    mod = import_module(module)
  File "C:\Users\allen\Envs\scrapy_py3\lib\importlib\__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 986, in _gcd_import
  File "<frozen importlib._bootstrap>", line 969, in _find_and_load
  File "<frozen importlib._bootstrap>", line 958, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 673, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 665, in exec_module
  File "<frozen importlib._bootstrap>", line 222, in _call_with_frames_removed
  File "E:\python-learning\scrapy\jd_spider\jd_spider\jd_spider\jd_spider\middlewares.py", line 24, in <module>
    IP_PORT_LIST = get_ip_port_list()
  File "E:\python-learning\scrapy\jd_spider\jd_spider\jd_spider\jd_spider\get_proxy.py", line 28, in get_ip_port_list
    r = requests.get('http://127.0.0.1:8000/?count=40')
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\adapters.py", line 508, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='127.0.0.1', port=8000): Max retries exceeded with url: /?count=40 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000022B08BC9B70>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。',))
2018-05-24 20:39:15 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: jd_spider)
2018-05-24 20:39:15 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (v3.5.2:4def2a2901a5, Jun 25 2016, 22:18:55) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 17.5.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-10-10.0.16299-SP0
2018-05-24 20:39:15 [scrapy.crawler] INFO: Overridden settings: {'SPIDER_LOADER_WARN_ONLY': True, 'NEWSPIDER_MODULE': 'jd_spider.spiders', 'LOG_FILE': 'JdSpider.log', 'SPIDER_MODULES': ['jd_spider.spiders'], 'DOWNLOAD_TIMEOUT': 30, 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'BOT_NAME': 'jd_spider', 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'LOG_LEVEL': 'INFO'}
2018-05-24 20:39:15 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.logstats.LogStats']
2018-05-24 20:39:15 [jd] INFO: Reading start URLs from redis key 'JdSpider:start_urls' (batch size: 16, encoding: utf-8
2018-05-24 20:39:17 [twisted] CRITICAL: Unhandled error in Deferred:
2018-05-24 20:39:17 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connection.py", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\util\connection.py", line 83, in create_connection
    raise err
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connectionpool.py", line 357, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "D:\Python35\Lib\http\client.py", line 1106, in request
    self._send_request(method, url, body, headers)
  File "D:\Python35\Lib\http\client.py", line 1151, in _send_request
    self.endheaders(body)
  File "D:\Python35\Lib\http\client.py", line 1102, in endheaders
    self._send_output(message_body)
  File "D:\Python35\Lib\http\client.py", line 934, in _send_output
    self.send(msg)
  File "D:\Python35\Lib\http\client.py", line 877, in send
    self.connect()
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connection.py", line 166, in connect
    conn = self._new_conn()
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connection.py", line 150, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000011EED4DAB70>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=8000): Max retries exceeded with url: /?count=40 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000011EED4DAB70>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。',))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\crawler.py", line 80, in crawl
    self.engine = self._create_engine()
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\crawler.py", line 105, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\engine.py", line 69, in __init__
    self.downloader = downloader_cls(crawler)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\__init__.py", line 88, in __init__
    self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\middleware.py", line 58, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\middleware.py", line 34, in from_settings
    mwcls = load_object(clspath)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\utils\misc.py", line 44, in load_object
    mod = import_module(module)
  File "C:\Users\allen\Envs\scrapy_py3\lib\importlib\__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 986, in _gcd_import
  File "<frozen importlib._bootstrap>", line 969, in _find_and_load
  File "<frozen importlib._bootstrap>", line 958, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 673, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 665, in exec_module
  File "<frozen importlib._bootstrap>", line 222, in _call_with_frames_removed
  File "E:\python-learning\scrapy\jd_spider\jd_spider\jd_spider\jd_spider\middlewares.py", line 24, in <module>
    IP_PORT_LIST = get_ip_port_list()
  File "E:\python-learning\scrapy\jd_spider\jd_spider\jd_spider\jd_spider\get_proxy.py", line 28, in get_ip_port_list
    r = requests.get('http://127.0.0.1:8000/?count=40')
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\adapters.py", line 508, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='127.0.0.1', port=8000): Max retries exceeded with url: /?count=40 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000011EED4DAB70>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。',))
2018-05-24 20:40:30 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: jd_spider)
2018-05-24 20:40:30 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (v3.5.2:4def2a2901a5, Jun 25 2016, 22:18:55) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 17.5.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-10-10.0.16299-SP0
2018-05-24 20:40:30 [scrapy.crawler] INFO: Overridden settings: {'SPIDER_MODULES': ['jd_spider.spiders'], 'SPIDER_LOADER_WARN_ONLY': True, 'BOT_NAME': 'jd_spider', 'DOWNLOAD_TIMEOUT': 30, 'NEWSPIDER_MODULE': 'jd_spider.spiders', 'LOG_FILE': 'JdSpider.log', 'LOG_LEVEL': 'INFO', 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler'}
2018-05-24 20:40:30 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-05-24 20:40:30 [jd] INFO: Reading start URLs from redis key 'JdSpider:start_urls' (batch size: 16, encoding: utf-8
2018-05-24 20:40:31 [twisted] CRITICAL: Unhandled error in Deferred:
2018-05-24 20:40:31 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connection.py", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\util\connection.py", line 83, in create_connection
    raise err
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connectionpool.py", line 357, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "D:\Python35\Lib\http\client.py", line 1106, in request
    self._send_request(method, url, body, headers)
  File "D:\Python35\Lib\http\client.py", line 1151, in _send_request
    self.endheaders(body)
  File "D:\Python35\Lib\http\client.py", line 1102, in endheaders
    self._send_output(message_body)
  File "D:\Python35\Lib\http\client.py", line 934, in _send_output
    self.send(msg)
  File "D:\Python35\Lib\http\client.py", line 877, in send
    self.connect()
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connection.py", line 166, in connect
    conn = self._new_conn()
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connection.py", line 150, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001D779559BE0>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=8000): Max retries exceeded with url: /?count=40 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001D779559BE0>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。',))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\crawler.py", line 80, in crawl
    self.engine = self._create_engine()
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\crawler.py", line 105, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\engine.py", line 69, in __init__
    self.downloader = downloader_cls(crawler)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\__init__.py", line 88, in __init__
    self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\middleware.py", line 58, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\middleware.py", line 34, in from_settings
    mwcls = load_object(clspath)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\utils\misc.py", line 44, in load_object
    mod = import_module(module)
  File "C:\Users\allen\Envs\scrapy_py3\lib\importlib\__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 986, in _gcd_import
  File "<frozen importlib._bootstrap>", line 969, in _find_and_load
  File "<frozen importlib._bootstrap>", line 958, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 673, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 665, in exec_module
  File "<frozen importlib._bootstrap>", line 222, in _call_with_frames_removed
  File "E:\python-learning\scrapy\jd_spider\jd_spider\jd_spider\jd_spider\middlewares.py", line 24, in <module>
    IP_PORT_LIST = get_ip_port_list()
  File "E:\python-learning\scrapy\jd_spider\jd_spider\jd_spider\jd_spider\get_proxy.py", line 28, in get_ip_port_list
    r = requests.get('http://127.0.0.1:8000/?count=40')
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\adapters.py", line 508, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='127.0.0.1', port=8000): Max retries exceeded with url: /?count=40 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001D779559BE0>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。',))
2018-05-24 20:40:41 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: jd_spider)
2018-05-24 20:40:41 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (v3.5.2:4def2a2901a5, Jun 25 2016, 22:18:55) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 17.5.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-10-10.0.16299-SP0
2018-05-24 20:40:41 [scrapy.crawler] INFO: Overridden settings: {'SPIDER_LOADER_WARN_ONLY': True, 'NEWSPIDER_MODULE': 'jd_spider.spiders', 'BOT_NAME': 'jd_spider', 'SPIDER_MODULES': ['jd_spider.spiders'], 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'DOWNLOAD_TIMEOUT': 30, 'LOG_FILE': 'JdSpider.log', 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'LOG_LEVEL': 'INFO'}
2018-05-24 20:40:41 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-05-24 20:40:41 [jd] INFO: Reading start URLs from redis key 'JdSpider:start_urls' (batch size: 16, encoding: utf-8
2018-05-24 20:40:42 [twisted] CRITICAL: Unhandled error in Deferred:
2018-05-24 20:40:42 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connection.py", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\util\connection.py", line 83, in create_connection
    raise err
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connectionpool.py", line 357, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "D:\Python35\Lib\http\client.py", line 1106, in request
    self._send_request(method, url, body, headers)
  File "D:\Python35\Lib\http\client.py", line 1151, in _send_request
    self.endheaders(body)
  File "D:\Python35\Lib\http\client.py", line 1102, in endheaders
    self._send_output(message_body)
  File "D:\Python35\Lib\http\client.py", line 934, in _send_output
    self.send(msg)
  File "D:\Python35\Lib\http\client.py", line 877, in send
    self.connect()
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connection.py", line 166, in connect
    conn = self._new_conn()
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connection.py", line 150, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001E2E4B855F8>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=8000): Max retries exceeded with url: /?count=40 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001E2E4B855F8>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。',))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\crawler.py", line 80, in crawl
    self.engine = self._create_engine()
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\crawler.py", line 105, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\engine.py", line 69, in __init__
    self.downloader = downloader_cls(crawler)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\__init__.py", line 88, in __init__
    self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\middleware.py", line 58, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\middleware.py", line 34, in from_settings
    mwcls = load_object(clspath)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\utils\misc.py", line 44, in load_object
    mod = import_module(module)
  File "C:\Users\allen\Envs\scrapy_py3\lib\importlib\__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 986, in _gcd_import
  File "<frozen importlib._bootstrap>", line 969, in _find_and_load
  File "<frozen importlib._bootstrap>", line 958, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 673, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 665, in exec_module
  File "<frozen importlib._bootstrap>", line 222, in _call_with_frames_removed
  File "E:\python-learning\scrapy\jd_spider\jd_spider\jd_spider\jd_spider\middlewares.py", line 24, in <module>
    IP_PORT_LIST = get_ip_port_list()
  File "E:\python-learning\scrapy\jd_spider\jd_spider\jd_spider\jd_spider\get_proxy.py", line 28, in get_ip_port_list
    r = requests.get('http://127.0.0.1:8000/?count=40')
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\adapters.py", line 508, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='127.0.0.1', port=8000): Max retries exceeded with url: /?count=40 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001E2E4B855F8>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。',))
2018-05-24 20:41:06 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: jd_spider)
2018-05-24 20:41:06 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (v3.5.2:4def2a2901a5, Jun 25 2016, 22:18:55) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 17.5.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-10-10.0.16299-SP0
2018-05-24 20:41:06 [scrapy.crawler] INFO: Overridden settings: {'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'NEWSPIDER_MODULE': 'jd_spider.spiders', 'BOT_NAME': 'jd_spider', 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'SPIDER_LOADER_WARN_ONLY': True, 'DOWNLOAD_TIMEOUT': 30, 'LOG_LEVEL': 'INFO', 'LOG_FILE': 'JdSpider.log', 'SPIDER_MODULES': ['jd_spider.spiders']}
2018-05-24 20:41:06 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2018-05-24 20:41:06 [jd] INFO: Reading start URLs from redis key 'JdSpider:start_urls' (batch size: 16, encoding: utf-8
2018-05-24 20:41:07 [twisted] CRITICAL: Unhandled error in Deferred:
2018-05-24 20:41:07 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connection.py", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\util\connection.py", line 83, in create_connection
    raise err
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connectionpool.py", line 357, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "D:\Python35\Lib\http\client.py", line 1106, in request
    self._send_request(method, url, body, headers)
  File "D:\Python35\Lib\http\client.py", line 1151, in _send_request
    self.endheaders(body)
  File "D:\Python35\Lib\http\client.py", line 1102, in endheaders
    self._send_output(message_body)
  File "D:\Python35\Lib\http\client.py", line 934, in _send_output
    self.send(msg)
  File "D:\Python35\Lib\http\client.py", line 877, in send
    self.connect()
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connection.py", line 166, in connect
    conn = self._new_conn()
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connection.py", line 150, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x00000216B3D68940>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=8000): Max retries exceeded with url: /?count=40 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000216B3D68940>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。',))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\crawler.py", line 80, in crawl
    self.engine = self._create_engine()
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\crawler.py", line 105, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\engine.py", line 69, in __init__
    self.downloader = downloader_cls(crawler)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\__init__.py", line 88, in __init__
    self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\middleware.py", line 58, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\middleware.py", line 34, in from_settings
    mwcls = load_object(clspath)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\utils\misc.py", line 44, in load_object
    mod = import_module(module)
  File "C:\Users\allen\Envs\scrapy_py3\lib\importlib\__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 986, in _gcd_import
  File "<frozen importlib._bootstrap>", line 969, in _find_and_load
  File "<frozen importlib._bootstrap>", line 958, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 673, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 665, in exec_module
  File "<frozen importlib._bootstrap>", line 222, in _call_with_frames_removed
  File "E:\python-learning\scrapy\jd_spider\jd_spider\jd_spider\jd_spider\middlewares.py", line 24, in <module>
    IP_PORT_LIST = get_ip_port_list()
  File "E:\python-learning\scrapy\jd_spider\jd_spider\jd_spider\jd_spider\get_proxy.py", line 28, in get_ip_port_list
    r = requests.get('http://127.0.0.1:8000/?count=40')
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\adapters.py", line 508, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='127.0.0.1', port=8000): Max retries exceeded with url: /?count=40 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000216B3D68940>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。',))
2018-05-24 20:42:29 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: jd_spider)
2018-05-24 20:42:29 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (v3.5.2:4def2a2901a5, Jun 25 2016, 22:18:55) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 17.5.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-10-10.0.16299-SP0
2018-05-24 20:42:29 [scrapy.crawler] INFO: Overridden settings: {'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'SPIDER_MODULES': ['jd_spider.spiders'], 'SPIDER_LOADER_WARN_ONLY': True, 'LOG_FILE': 'JdSpider.log', 'LOG_LEVEL': 'INFO', 'DOWNLOAD_TIMEOUT': 30, 'NEWSPIDER_MODULE': 'jd_spider.spiders', 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'BOT_NAME': 'jd_spider'}
2018-05-24 20:42:29 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-05-24 20:42:29 [jd] INFO: Reading start URLs from redis key 'JdSpider:start_urls' (batch size: 16, encoding: utf-8
2018-05-24 20:42:30 [twisted] CRITICAL: Unhandled error in Deferred:
2018-05-24 20:42:30 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connection.py", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\util\connection.py", line 83, in create_connection
    raise err
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connectionpool.py", line 357, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "D:\Python35\Lib\http\client.py", line 1106, in request
    self._send_request(method, url, body, headers)
  File "D:\Python35\Lib\http\client.py", line 1151, in _send_request
    self.endheaders(body)
  File "D:\Python35\Lib\http\client.py", line 1102, in endheaders
    self._send_output(message_body)
  File "D:\Python35\Lib\http\client.py", line 934, in _send_output
    self.send(msg)
  File "D:\Python35\Lib\http\client.py", line 877, in send
    self.connect()
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connection.py", line 166, in connect
    conn = self._new_conn()
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connection.py", line 150, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x00000291EA2D6940>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=8000): Max retries exceeded with url: /?count=40 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000291EA2D6940>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。',))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\crawler.py", line 80, in crawl
    self.engine = self._create_engine()
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\crawler.py", line 105, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\engine.py", line 69, in __init__
    self.downloader = downloader_cls(crawler)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\__init__.py", line 88, in __init__
    self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\middleware.py", line 58, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\middleware.py", line 34, in from_settings
    mwcls = load_object(clspath)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\utils\misc.py", line 44, in load_object
    mod = import_module(module)
  File "C:\Users\allen\Envs\scrapy_py3\lib\importlib\__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 986, in _gcd_import
  File "<frozen importlib._bootstrap>", line 969, in _find_and_load
  File "<frozen importlib._bootstrap>", line 958, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 673, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 665, in exec_module
  File "<frozen importlib._bootstrap>", line 222, in _call_with_frames_removed
  File "E:\python-learning\scrapy\jd_spider\jd_spider\jd_spider\jd_spider\middlewares.py", line 24, in <module>
    IP_PORT_LIST = get_ip_port_list()
  File "E:\python-learning\scrapy\jd_spider\jd_spider\jd_spider\jd_spider\get_proxy.py", line 28, in get_ip_port_list
    r = requests.get('http://127.0.0.1:8000/?count=40')
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\adapters.py", line 508, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='127.0.0.1', port=8000): Max retries exceeded with url: /?count=40 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000291EA2D6940>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。',))
2018-05-24 20:42:34 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: jd_spider)
2018-05-24 20:42:34 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (v3.5.2:4def2a2901a5, Jun 25 2016, 22:18:55) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 17.5.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-10-10.0.16299-SP0
2018-05-24 20:42:34 [scrapy.crawler] INFO: Overridden settings: {'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'SPIDER_MODULES': ['jd_spider.spiders'], 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'jd_spider', 'SPIDER_LOADER_WARN_ONLY': True, 'NEWSPIDER_MODULE': 'jd_spider.spiders', 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'LOG_FILE': 'JdSpider.log', 'DOWNLOAD_TIMEOUT': 30}
2018-05-24 20:42:34 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2018-05-24 20:42:34 [jd] INFO: Reading start URLs from redis key 'JdSpider:start_urls' (batch size: 16, encoding: utf-8
2018-05-24 20:42:35 [twisted] CRITICAL: Unhandled error in Deferred:
2018-05-24 20:42:35 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connection.py", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\util\connection.py", line 83, in create_connection
    raise err
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connectionpool.py", line 357, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "D:\Python35\Lib\http\client.py", line 1106, in request
    self._send_request(method, url, body, headers)
  File "D:\Python35\Lib\http\client.py", line 1151, in _send_request
    self.endheaders(body)
  File "D:\Python35\Lib\http\client.py", line 1102, in endheaders
    self._send_output(message_body)
  File "D:\Python35\Lib\http\client.py", line 934, in _send_output
    self.send(msg)
  File "D:\Python35\Lib\http\client.py", line 877, in send
    self.connect()
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connection.py", line 166, in connect
    conn = self._new_conn()
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connection.py", line 150, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x00000144E4CE6940>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=8000): Max retries exceeded with url: /?count=40 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000144E4CE6940>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。',))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\crawler.py", line 80, in crawl
    self.engine = self._create_engine()
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\crawler.py", line 105, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\engine.py", line 69, in __init__
    self.downloader = downloader_cls(crawler)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\__init__.py", line 88, in __init__
    self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\middleware.py", line 58, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\middleware.py", line 34, in from_settings
    mwcls = load_object(clspath)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\utils\misc.py", line 44, in load_object
    mod = import_module(module)
  File "C:\Users\allen\Envs\scrapy_py3\lib\importlib\__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 986, in _gcd_import
  File "<frozen importlib._bootstrap>", line 969, in _find_and_load
  File "<frozen importlib._bootstrap>", line 958, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 673, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 665, in exec_module
  File "<frozen importlib._bootstrap>", line 222, in _call_with_frames_removed
  File "E:\python-learning\scrapy\jd_spider\jd_spider\jd_spider\jd_spider\middlewares.py", line 24, in <module>
    IP_PORT_LIST = get_ip_port_list()
  File "E:\python-learning\scrapy\jd_spider\jd_spider\jd_spider\jd_spider\get_proxy.py", line 28, in get_ip_port_list
    r = requests.get('http://127.0.0.1:8000/?count=40')
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\requests\adapters.py", line 508, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='127.0.0.1', port=8000): Max retries exceeded with url: /?count=40 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000144E4CE6940>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。',))
2018-05-24 20:46:05 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: jd_spider)
2018-05-24 20:46:05 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (v3.5.2:4def2a2901a5, Jun 25 2016, 22:18:55) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 17.5.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-10-10.0.16299-SP0
2018-05-24 20:46:05 [scrapy.crawler] INFO: Overridden settings: {'SPIDER_LOADER_WARN_ONLY': True, 'DOWNLOAD_TIMEOUT': 30, 'SPIDER_MODULES': ['jd_spider.spiders'], 'BOT_NAME': 'jd_spider', 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'NEWSPIDER_MODULE': 'jd_spider.spiders', 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'LOG_LEVEL': 'INFO', 'LOG_FILE': 'JdSpider.log'}
2018-05-24 20:46:05 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.corestats.CoreStats']
2018-05-24 20:46:05 [jd] INFO: Reading start URLs from redis key 'JdSpider:start_urls' (batch size: 16, encoding: utf-8
2018-05-24 20:46:05 [scrapy.middleware] INFO: Enabled downloader middlewares:
['jd_spider.middlewares.JdSpiderDownloaderMiddleware',
 'jd_spider.middlewares.RandomProxy',
 'jd_spider.middlewares.MyRetryMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-05-24 20:46:05 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-05-24 20:46:05 [scrapy.middleware] INFO: Enabled item pipelines:
['scrapy_redis.pipelines.RedisPipeline']
2018-05-24 20:46:05 [scrapy.core.engine] INFO: Spider opened
2018-05-24 20:46:05 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-05-24 20:46:05 [jd] INFO: Spider opened: jd
2018-05-24 20:47:05 [scrapy.extensions.logstats] INFO: Crawled 4212 pages (at 4212 pages/min), scraped 1 items (at 1 items/min)
2018-05-24 21:01:18 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: jd_spider)
2018-05-24 21:01:18 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (v3.5.2:4def2a2901a5, Jun 25 2016, 22:18:55) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 17.5.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-10-10.0.16299-SP0
2018-05-24 21:01:18 [scrapy.crawler] INFO: Overridden settings: {'DOWNLOAD_TIMEOUT': 30, 'NEWSPIDER_MODULE': 'jd_spider.spiders', 'BOT_NAME': 'jd_spider', 'SPIDER_LOADER_WARN_ONLY': True, 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'LOG_FILE': 'JdSpider.log', 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'LOG_LEVEL': 'INFO', 'SPIDER_MODULES': ['jd_spider.spiders']}
2018-05-24 21:01:18 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.logstats.LogStats']
2018-05-24 21:01:18 [jd] INFO: Reading start URLs from redis key 'JdSpider:start_urls' (batch size: 16, encoding: utf-8
2018-05-24 21:01:18 [scrapy.middleware] INFO: Enabled downloader middlewares:
['jd_spider.middlewares.JdSpiderDownloaderMiddleware',
 'jd_spider.middlewares.RandomProxy',
 'jd_spider.middlewares.MyRetryMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-05-24 21:01:18 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-05-24 21:01:18 [scrapy.middleware] INFO: Enabled item pipelines:
['scrapy_redis.pipelines.RedisPipeline']
2018-05-24 21:01:18 [scrapy.core.engine] INFO: Spider opened
2018-05-24 21:01:18 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-05-24 21:01:18 [jd] INFO: Spider opened: jd
2018-05-24 21:02:02 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_13046091845&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129690807>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-05-24 21:02:18 [scrapy.extensions.logstats] INFO: Crawled 33 pages (at 33 pages/min), scraped 0 items (at 0 items/min)
2018-05-24 21:02:33 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_21680343693&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129690791>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:02:34 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_17200197566&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129690838>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2018-05-24 21:02:37 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_14419736828&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129690807>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2018-05-24 21:02:38 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_11140338708&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129690776>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:02:55 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_11466246417&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129690807>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:02:56 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_18661151510&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129690807>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2018-05-24 21:03:01 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://radius.inetm.pl/notification took longer than 30.0 seconds..
2018-05-24 21:03:08 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_10516246240&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129690338>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:03:09 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_10123419712&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129690776>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:03:09 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_11467022466&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129690807>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-05-24 21:03:17 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_11466246420&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129690822>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:03:17 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-05-24 21:03:18 [scrapy.extensions.logstats] INFO: Crawled 76 pages (at 43 pages/min), scraped 34 items (at 34 items/min)
2018-05-24 21:03:38 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_10123407865&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129690776>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:03:57 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_10512939652&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129690432>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2018-05-24 21:04:12 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://radius.inetm.pl/notification took longer than 30.0 seconds..
2018-05-24 21:04:18 [scrapy.extensions.logstats] INFO: Crawled 117 pages (at 41 pages/min), scraped 65 items (at 31 items/min)
2018-05-24 21:04:33 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_11140072804&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129690432>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2018-05-24 21:04:44 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_1630636268&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129689197>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:04:45 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://radius.inetm.pl/notification took longer than 30.0 seconds..
2018-05-24 21:05:12 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_25731826834&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129689916>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:05:17 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_11512142018&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129690838>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:05:18 [scrapy.extensions.logstats] INFO: Crawled 159 pages (at 42 pages/min), scraped 67 items (at 2 items/min)
2018-05-24 21:05:23 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_11467022493&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129690807>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_11467022493&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129690807 took longer than 30.0 seconds..
2018-05-24 21:05:32 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_10122431433&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129689994>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:05:33 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_10123409022&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129690385>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:05:36 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_11933119809&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129690432>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:05:38 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_21673559807&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129690385>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:05:57 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://radius.inetm.pl/notification took longer than 30.0 seconds..
2018-05-24 21:05:59 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_11140214861&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129690385>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:06:18 [scrapy.extensions.logstats] INFO: Crawled 183 pages (at 24 pages/min), scraped 68 items (at 1 items/min)
2018-05-24 21:06:21 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_10123409023&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129690385>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:06:22 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_11933103219&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129690385>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:06:36 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://radius.inetm.pl/notification took longer than 30.0 seconds..
2018-05-24 21:06:36 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:06:56 [scrapy.core.scraper] ERROR: Spider error processing <GET http://radius.inetm.pl/notification> (referer: http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_11139674462&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129690369)
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\python-learning\scrapy\jd_spider\jd_spider\jd_spider\jd_spider\spiders\jd.py", line 70, in parse_price
    json_obj = json.loads(response.text)
  File "D:\Python35\Lib\json\__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "D:\Python35\Lib\json\decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "D:\Python35\Lib\json\decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
2018-05-24 21:07:02 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_10123413127&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129690385>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:07:15 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_10123416224&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129690385>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_10123416224&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129690385 took longer than 30.0 seconds..
2018-05-24 21:07:18 [scrapy.extensions.logstats] INFO: Crawled 238 pages (at 55 pages/min), scraped 70 items (at 2 items/min)
2018-05-24 21:07:34 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_10123416223&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129690369>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:07:45 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_10154088293&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129690369>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:07:47 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_11139320753&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129690369>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:07:53 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_10123413128&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129690385>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:08:11 [scrapy.core.scraper] ERROR: Spider error processing <GET http://radius.inetm.pl/notification> (referer: http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_25907500425&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129690369)
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\python-learning\scrapy\jd_spider\jd_spider\jd_spider\jd_spider\spiders\jd.py", line 70, in parse_price
    json_obj = json.loads(response.text)
  File "D:\Python35\Lib\json\__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "D:\Python35\Lib\json\decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "D:\Python35\Lib\json\decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
2018-05-24 21:08:14 [scrapy.core.scraper] ERROR: Spider error processing <GET http://radius.inetm.pl/notification> (referer: http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_11381590502&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129688307)
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\python-learning\scrapy\jd_spider\jd_spider\jd_spider\jd_spider\spiders\jd.py", line 70, in parse_price
    json_obj = json.loads(response.text)
  File "D:\Python35\Lib\json\__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "D:\Python35\Lib\json\decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "D:\Python35\Lib\json\decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
2018-05-24 21:08:18 [scrapy.extensions.logstats] INFO: Crawled 281 pages (at 43 pages/min), scraped 70 items (at 0 items/min)
2018-05-24 21:08:28 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_25908334732&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129690354>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2018-05-24 21:08:30 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_10516246239&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129690354>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:08:30 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_10512872038&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129690369>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2018-05-24 21:08:40 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_10101676216&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129690369>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:08:41 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2018-05-24 21:08:51 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_10123407748&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129690369>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:08:51 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_10154077798&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129690354>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:09:11 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_11096686905&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129689182>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:09:17 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_10154082123&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129690338>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:09:18 [scrapy.extensions.logstats] INFO: Crawled 350 pages (at 69 pages/min), scraped 136 items (at 66 items/min)
2018-05-24 21:09:47 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_25908215409&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129690338>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-05-24 21:10:10 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_13509379850&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129690322>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:10:16 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_13509379851&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129690322>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:10:18 [scrapy.extensions.logstats] INFO: Crawled 411 pages (at 61 pages/min), scraped 189 items (at 53 items/min)
2018-05-24 21:10:24 [scrapy.core.scraper] ERROR: Error downloading <GET http://club.jd.com/comment/productCommentSummaries.action?referenceIds=22928432071&_=1527167372169>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://club.jd.com/comment/productCommentSummaries.action?referenceIds=22928432071&_=1527167372169 took longer than 30.0 seconds..
2018-05-24 21:10:46 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://radius.inetm.pl/notification took longer than 30.0 seconds..
2018-05-24 21:10:57 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_1510947663&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129689947>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:11:07 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_11139607088&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129689979>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:11:17 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_11139607089&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129689979>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:11:18 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_11139320754&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129689979>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:11:18 [scrapy.extensions.logstats] INFO: Crawled 470 pages (at 59 pages/min), scraped 277 items (at 88 items/min)
2018-05-24 21:11:24 [scrapy.core.scraper] ERROR: Error downloading <GET http://club.jd.com/comment/productCommentSummaries.action?referenceIds=1707108270&_=1527129689963>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-05-24 21:11:28 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_23390227609&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129687775>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:11:31 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_10122431434&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129689994>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2018-05-24 21:11:35 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_10122431429&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129689979>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:12:01 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://radius.inetm.pl/notification took longer than 30.0 seconds..
2018-05-24 21:12:08 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_1662340241&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129689947>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:12:18 [scrapy.extensions.logstats] INFO: Crawled 507 pages (at 37 pages/min), scraped 300 items (at 23 items/min)
2018-05-24 21:12:33 [scrapy.core.scraper] ERROR: Spider error processing <GET http://radius.inetm.pl/notification> (referer: http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_1662340240&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129689947)
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\python-learning\scrapy\jd_spider\jd_spider\jd_spider\jd_spider\spiders\jd.py", line 70, in parse_price
    json_obj = json.loads(response.text)
  File "D:\Python35\Lib\json\__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "D:\Python35\Lib\json\decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "D:\Python35\Lib\json\decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
2018-05-24 21:12:54 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_1510947653&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129689947>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:13:07 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_1510947662&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129689947>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:13:13 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_13567939715&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129689916>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:13:15 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_1510947652&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129689947>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:13:15 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_1510947654&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129689947>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:13:17 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://radius.inetm.pl/notification took longer than 30.0 seconds..
2018-05-24 21:13:18 [scrapy.extensions.logstats] INFO: Crawled 576 pages (at 69 pages/min), scraped 426 items (at 126 items/min)
2018-05-24 21:13:37 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://radius.inetm.pl/notification took longer than 30.0 seconds..
2018-05-24 21:13:38 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_12888062665&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129689916>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:13:55 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:14:04 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_10100955459&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129689947>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:14:18 [scrapy.extensions.logstats] INFO: Crawled 650 pages (at 74 pages/min), scraped 619 items (at 193 items/min)
2018-05-24 21:14:37 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_12437017094&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129689916>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:14:53 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_13569444156&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129689900>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:14:55 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://radius.inetm.pl/notification took longer than 30.0 seconds..
2018-05-24 21:15:14 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_12434445086&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129689182>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:15:18 [scrapy.extensions.logstats] INFO: Crawled 717 pages (at 67 pages/min), scraped 707 items (at 88 items/min)
2018-05-24 21:15:19 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_10469219709&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129689900>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:15:27 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_12405701280&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129689182>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:15:31 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_12434445097&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129689182>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:16:18 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-05-24 21:16:18 [scrapy.extensions.logstats] INFO: Crawled 772 pages (at 55 pages/min), scraped 729 items (at 22 items/min)
2018-05-24 21:16:29 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:16:34 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_21653513364&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129686650>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:16:53 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_11726803211&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129687291>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:17:12 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_12400290574&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129689182>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:17:14 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://radius.inetm.pl/notification took longer than 30.0 seconds..
2018-05-24 21:17:18 [scrapy.extensions.logstats] INFO: Crawled 818 pages (at 46 pages/min), scraped 752 items (at 23 items/min)
2018-05-24 21:17:21 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_12400290584&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129689166>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:17:35 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_11102683131&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129689166>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:17:36 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_12436080840&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129689166>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:17:41 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://radius.inetm.pl/notification took longer than 30.0 seconds..
2018-05-24 21:17:57 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_12414755178&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129689166>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:18:07 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_12434445096&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129689182>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:18:17 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_11728643359&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129687697>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2018-05-24 21:18:18 [scrapy.extensions.logstats] INFO: Crawled 887 pages (at 69 pages/min), scraped 796 items (at 44 items/min)
2018-05-24 21:18:23 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_26128434653&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129689150>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:19:15 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_1560525654&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129689166>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:19:18 [scrapy.extensions.logstats] INFO: Crawled 971 pages (at 84 pages/min), scraped 853 items (at 57 items/min)
2018-05-24 21:19:26 [scrapy.core.scraper] ERROR: Spider error processing <GET http://radius.inetm.pl/notification> (referer: http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_23390213586&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129687775)
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\python-learning\scrapy\jd_spider\jd_spider\jd_spider\jd_spider\spiders\jd.py", line 70, in parse_price
    json_obj = json.loads(response.text)
  File "D:\Python35\Lib\json\__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "D:\Python35\Lib\json\decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "D:\Python35\Lib\json\decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
2018-05-24 21:19:29 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://radius.inetm.pl/notification took longer than 30.0 seconds..
2018-05-24 21:19:38 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_11325632867&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129689135>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:20:01 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_11319197752&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129689150>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_11319197752&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129689150 took longer than 30.0 seconds..
2018-05-24 21:20:18 [scrapy.extensions.logstats] INFO: Crawled 1042 pages (at 71 pages/min), scraped 895 items (at 42 items/min)
2018-05-24 21:20:21 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-05-24 21:20:32 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://radius.inetm.pl/notification took longer than 30.0 seconds..
2018-05-24 21:20:33 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://radius.inetm.pl/notification took longer than 30.0 seconds..
2018-05-24 21:20:48 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_11155365055&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129689119>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:20:49 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2018-05-24 21:21:18 [scrapy.extensions.logstats] INFO: Crawled 1151 pages (at 109 pages/min), scraped 1051 items (at 156 items/min)
2018-05-24 21:21:28 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_20746457708&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129689104>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:21:40 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_10472335333&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129689104>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:21:48 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_12580818468&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129688791>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-05-24 21:21:58 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-05-24 21:22:04 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://radius.inetm.pl/notification took longer than 30.0 seconds..
2018-05-24 21:22:10 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://radius.inetm.pl/notification took longer than 30.0 seconds..
2018-05-24 21:22:18 [scrapy.extensions.logstats] INFO: Crawled 1224 pages (at 73 pages/min), scraped 1051 items (at 0 items/min)
2018-05-24 21:23:07 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_11749198405&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129688791>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:23:12 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_12580818467&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129688791>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:23:18 [scrapy.extensions.logstats] INFO: Crawled 1311 pages (at 87 pages/min), scraped 1143 items (at 92 items/min)
2018-05-24 21:23:32 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_11381590514&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129688791>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:23:46 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_11381582794&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129688791>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:23:47 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://radius.inetm.pl/notification took longer than 30.0 seconds..
2018-05-24 21:23:51 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_11382867640&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129688760>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2018-05-24 21:24:02 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://radius.inetm.pl/notification took longer than 30.0 seconds..
2018-05-24 21:24:08 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_12580818480&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129688760>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:24:18 [scrapy.extensions.logstats] INFO: Crawled 1419 pages (at 108 pages/min), scraped 1217 items (at 74 items/min)
2018-05-24 21:24:20 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_12580818466&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129688760>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:24:23 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:24:26 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_11381590520&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129688791>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:24:48 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_12580818477&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129688760>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:25:01 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_10466015679&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129688697>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2018-05-24 21:25:02 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_10157894593&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129688697>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:25:12 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://radius.inetm.pl/notification took longer than 30.0 seconds..
2018-05-24 21:25:18 [scrapy.extensions.logstats] INFO: Crawled 1558 pages (at 139 pages/min), scraped 1680 items (at 463 items/min)
2018-05-24 21:25:36 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_12580818481&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129688760>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2018-05-24 21:26:04 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://radius.inetm.pl/notification took longer than 30.0 seconds..
2018-05-24 21:26:04 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://radius.inetm.pl/notification took longer than 30.0 seconds..
2018-05-24 21:26:18 [scrapy.extensions.logstats] INFO: Crawled 1725 pages (at 167 pages/min), scraped 2099 items (at 419 items/min)
2018-05-24 21:26:45 [scrapy.core.scraper] ERROR: Spider error processing <GET http://radius.inetm.pl/notification> (referer: http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_10157894587&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129688697)
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\python-learning\scrapy\jd_spider\jd_spider\jd_spider\jd_spider\spiders\jd.py", line 70, in parse_price
    json_obj = json.loads(response.text)
  File "D:\Python35\Lib\json\__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "D:\Python35\Lib\json\decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "D:\Python35\Lib\json\decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
2018-05-24 21:27:18 [scrapy.extensions.logstats] INFO: Crawled 1960 pages (at 235 pages/min), scraped 2953 items (at 854 items/min)
2018-05-24 21:27:33 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://radius.inetm.pl/notification took longer than 30.0 seconds..
2018-05-24 21:27:41 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://radius.inetm.pl/notification took longer than 30.0 seconds..
2018-05-24 21:27:59 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_27656829825&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129688307>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:28:08 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://radius.inetm.pl/notification took longer than 30.0 seconds..
2018-05-24 21:28:18 [scrapy.extensions.logstats] INFO: Crawled 2103 pages (at 143 pages/min), scraped 3100 items (at 147 items/min)
2018-05-24 21:28:56 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://radius.inetm.pl/notification took longer than 30.0 seconds..
2018-05-24 21:29:12 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2018-05-24 21:29:18 [scrapy.extensions.logstats] INFO: Crawled 2171 pages (at 68 pages/min), scraped 3196 items (at 96 items/min)
2018-05-24 21:29:20 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://radius.inetm.pl/notification took longer than 30.0 seconds..
2018-05-24 21:29:27 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://radius.inetm.pl/notification took longer than 30.0 seconds..
2018-05-24 21:29:43 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://radius.inetm.pl/notification took longer than 30.0 seconds..
2018-05-24 21:29:50 [scrapy.core.scraper] ERROR: Spider error processing <GET http://radius.inetm.pl/notification> (referer: http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_23390213596&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129687760)
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\python-learning\scrapy\jd_spider\jd_spider\jd_spider\jd_spider\spiders\jd.py", line 70, in parse_price
    json_obj = json.loads(response.text)
  File "D:\Python35\Lib\json\__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "D:\Python35\Lib\json\decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "D:\Python35\Lib\json\decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
2018-05-24 21:29:50 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://radius.inetm.pl/notification took longer than 30.0 seconds..
2018-05-24 21:30:00 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_23390227603&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129688166>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2018-05-24 21:30:04 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_12986885224&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129687760>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2018-05-24 21:30:10 [scrapy.core.scraper] ERROR: Spider error processing <GET http://radius.inetm.pl/notification> (referer: http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_23390227610&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129688166)
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\python-learning\scrapy\jd_spider\jd_spider\jd_spider\jd_spider\spiders\jd.py", line 70, in parse_price
    json_obj = json.loads(response.text)
  File "D:\Python35\Lib\json\__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "D:\Python35\Lib\json\decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "D:\Python35\Lib\json\decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
2018-05-24 21:30:18 [scrapy.extensions.logstats] INFO: Crawled 2252 pages (at 81 pages/min), scraped 3214 items (at 18 items/min)
2018-05-24 21:30:32 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://radius.inetm.pl/notification took longer than 30.0 seconds..
2018-05-24 21:30:34 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_23390227604&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129688182>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2018-05-24 21:30:38 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_11727563425&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129687760>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2018-05-24 21:30:40 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://radius.inetm.pl/notification took longer than 30.0 seconds..
2018-05-24 21:30:47 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_11727563431&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129687744>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-05-24 21:30:54 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_11727563432&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129687744>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2018-05-24 21:31:06 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://radius.inetm.pl/notification took longer than 30.0 seconds..
2018-05-24 21:31:08 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_23390213589&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129688182>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:31:18 [scrapy.extensions.logstats] INFO: Crawled 2355 pages (at 103 pages/min), scraped 3221 items (at 7 items/min)
2018-05-24 21:31:32 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_10502032036&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129687744>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:31:52 [scrapy.core.scraper] ERROR: Spider error processing <GET http://radius.inetm.pl/notification> (referer: http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_25325793924&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129687728)
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\python-learning\scrapy\jd_spider\jd_spider\jd_spider\jd_spider\spiders\jd.py", line 70, in parse_price
    json_obj = json.loads(response.text)
  File "D:\Python35\Lib\json\__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "D:\Python35\Lib\json\decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "D:\Python35\Lib\json\decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
2018-05-24 21:32:18 [scrapy.extensions.logstats] INFO: Crawled 2449 pages (at 94 pages/min), scraped 3241 items (at 20 items/min)
2018-05-24 21:32:22 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://radius.inetm.pl/notification took longer than 30.0 seconds..
2018-05-24 21:32:24 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2018-05-24 21:32:40 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_25325793923&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129687744>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:32:48 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_11728454545&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129687713>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:32:50 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_10502032035&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129687713>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:33:02 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://radius.inetm.pl/notification took longer than 30.0 seconds..
2018-05-24 21:33:03 [scrapy.core.scraper] ERROR: Spider error processing <GET http://radius.inetm.pl/notification> (referer: http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_21653513329&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129686588)
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\python-learning\scrapy\jd_spider\jd_spider\jd_spider\jd_spider\spiders\jd.py", line 70, in parse_price
    json_obj = json.loads(response.text)
  File "D:\Python35\Lib\json\__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "D:\Python35\Lib\json\decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "D:\Python35\Lib\json\decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
2018-05-24 21:33:08 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:33:10 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_14448257394&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129687728>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:33:18 [scrapy.extensions.logstats] INFO: Crawled 2561 pages (at 112 pages/min), scraped 3353 items (at 112 items/min)
2018-05-24 21:33:58 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://radius.inetm.pl/notification took longer than 30.0 seconds..
2018-05-24 21:34:18 [scrapy.extensions.logstats] INFO: Crawled 2642 pages (at 81 pages/min), scraped 3415 items (at 62 items/min)
2018-05-24 21:34:30 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://radius.inetm.pl/notification took longer than 30.0 seconds..
2018-05-24 21:35:04 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_21653513403&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129686650>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2018-05-24 21:35:18 [scrapy.extensions.logstats] INFO: Crawled 2745 pages (at 103 pages/min), scraped 3467 items (at 52 items/min)
2018-05-24 21:35:22 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://radius.inetm.pl/notification took longer than 30.0 seconds..
2018-05-24 21:35:31 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://radius.inetm.pl/notification took longer than 30.0 seconds..
2018-05-24 21:35:41 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://radius.inetm.pl/notification took longer than 30.0 seconds..
2018-05-24 21:35:42 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_10393304486&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129687260>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2018-05-24 21:35:48 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://radius.inetm.pl/notification took longer than 30.0 seconds..
2018-05-24 21:35:55 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_11881753170&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129687260>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:36:01 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_10459713572&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129687260>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:36:13 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://radius.inetm.pl/notification took longer than 30.0 seconds..
2018-05-24 21:36:18 [scrapy.extensions.logstats] INFO: Crawled 2861 pages (at 116 pages/min), scraped 3736 items (at 269 items/min)
2018-05-24 21:36:57 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_25324746252&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129687228>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:37:01 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_12986874798&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129687228>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:37:18 [scrapy.extensions.logstats] INFO: Crawled 2934 pages (at 73 pages/min), scraped 3769 items (at 33 items/min)
2018-05-24 21:37:20 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_11728643363&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129687697>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2018-05-24 21:37:36 [scrapy.core.scraper] ERROR: Error downloading <GET http://download.2345.com/jifen_browser/2345_k1875408_browser.exe>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:37:40 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_21655168590&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129686744>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:37:40 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_21388158427&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129687197>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:38:18 [scrapy.extensions.logstats] INFO: Crawled 3003 pages (at 69 pages/min), scraped 3852 items (at 83 items/min)
2018-05-24 21:38:18 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_21471728071&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129687181>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:38:36 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_21388158433&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129687197>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2018-05-24 21:38:40 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_10393304468&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129687291>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2018-05-24 21:38:43 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_21478718899&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129687197>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:38:44 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_21471728064&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129687181>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:38:48 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:38:58 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_25324840453&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129687181>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:39:05 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://radius.inetm.pl/notification took longer than 30.0 seconds..
2018-05-24 21:39:08 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://radius.inetm.pl/notification took longer than 30.0 seconds..
2018-05-24 21:39:18 [scrapy.extensions.logstats] INFO: Crawled 3039 pages (at 36 pages/min), scraped 3854 items (at 2 items/min)
2018-05-24 21:39:59 [scrapy.core.scraper] ERROR: Error downloading <GET http://download.2345.com/jifen_browser/2345_k1875408_browser.exe>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:40:03 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_21471728068&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129687166>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:40:18 [scrapy.extensions.logstats] INFO: Crawled 3068 pages (at 29 pages/min), scraped 3867 items (at 13 items/min)
2018-05-24 21:40:27 [scrapy.core.scraper] ERROR: Error downloading <GET http://183.91.33.92/download.2345.com/jifen_browser/2345_k1875408_browser.exe>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:40:29 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_21655168563&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129687166>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:40:31 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_21471728070&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129687181>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2018-05-24 21:40:36 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_21655168575&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129687166>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:40:37 [scrapy.core.scraper] ERROR: Error downloading <GET http://club.jd.com/comment/productCommentSummaries.action?referenceIds=21655168571&_=1527169173937>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:40:49 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_21655168584&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129686728>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2018-05-24 21:40:52 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:40:58 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_21655168592&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129686728>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:40:58 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_7154696&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129685306>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:41:13 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_21655168574&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129687150>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:41:15 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_21653513355&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129686103>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:41:18 [scrapy.extensions.logstats] INFO: Crawled 3089 pages (at 21 pages/min), scraped 3868 items (at 1 items/min)
2018-05-24 21:41:34 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_21655168569&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129687150>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:42:03 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_21655168566&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129686713>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2018-05-24 21:42:09 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_21653513338&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129686713>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_21653513338&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129686713 took longer than 30.0 seconds..
2018-05-24 21:42:14 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_21653513360&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129686650>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2018-05-24 21:42:18 [scrapy.extensions.logstats] INFO: Crawled 3114 pages (at 25 pages/min), scraped 3870 items (at 2 items/min)
2018-05-24 21:42:21 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_21653513368&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129686713>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2018-05-24 21:42:35 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_21653513318&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129686713>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:42:49 [scrapy.core.scraper] ERROR: Error downloading <GET http://club.jd.com/comment/productCommentSummaries.action?referenceIds=21653513373&_=1527169299974>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:42:53 [scrapy.core.scraper] ERROR: Error downloading <GET http://download.2345.com/jifen_browser/2345_k1875408_browser.exe>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:42:58 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_21653513363&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129686603>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:43:00 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_21653513324&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129686603>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:43:18 [scrapy.extensions.logstats] INFO: Crawled 3143 pages (at 29 pages/min), scraped 3883 items (at 13 items/min)
2018-05-24 21:43:37 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://radius.inetm.pl/notification took longer than 30.0 seconds..
2018-05-24 21:43:58 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_21653513330&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129686603>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:44:02 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_21653513307&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129686588>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:44:18 [scrapy.extensions.logstats] INFO: Crawled 3168 pages (at 25 pages/min), scraped 3906 items (at 23 items/min)
2018-05-24 21:44:19 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_21653513352&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129686572>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:44:30 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_21653513391&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129686588>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:44:51 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_21653513407&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129686572>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:44:51 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_21653513353&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129686572>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:45:14 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://radius.inetm.pl/notification took longer than 30.0 seconds..
2018-05-24 21:45:18 [scrapy.extensions.logstats] INFO: Crawled 3206 pages (at 38 pages/min), scraped 4050 items (at 144 items/min)
2018-05-24 21:45:19 [scrapy.core.scraper] ERROR: Error downloading <GET http://sclub.jd.com/comment/productPageComments.action?productId=12580818491&score=0&sortType=5&page=1&pageSize=10>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:45:27 [scrapy.core.scraper] ERROR: Error downloading <GET http://download.2345.com/jifen_browser/2345_k1875408_browser.exe>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:45:33 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://radius.inetm.pl/notification took longer than 30.0 seconds..
2018-05-24 21:45:37 [scrapy.core.scraper] ERROR: Error downloading <GET http://sclub.jd.com/comment/productPageComments.action?productId=10157894591&score=0&sortType=5&page=1&pageSize=10>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:45:39 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_21653513323&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129686572>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:45:41 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_5373190&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129685306>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:45:42 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://radius.inetm.pl/notification took longer than 30.0 seconds..
2018-05-24 21:45:56 [scrapy.core.scraper] ERROR: Error downloading <GET http://club.jd.com/comment/productCommentSummaries.action?referenceIds=26756066117&_=1527169503904>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:46:18 [scrapy.extensions.logstats] INFO: Crawled 3241 pages (at 35 pages/min), scraped 4074 items (at 24 items/min)
2018-05-24 21:46:25 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_21653513350&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129686088>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:46:28 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_21653513376&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129686088>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-05-24 21:46:56 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_10393304483&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129686088>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:47:07 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_11096686937&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129686056>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:47:17 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_11155365057&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129686056>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:47:18 [scrapy.extensions.logstats] INFO: Crawled 3285 pages (at 44 pages/min), scraped 4151 items (at 77 items/min)
2018-05-24 21:47:21 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_21653513370&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129686088>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2018-05-24 21:47:22 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_7368337&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129685306>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:47:25 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_10393304485&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129686088>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:47:25 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_21653513397&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129686088>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_21653513397&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129686088 took longer than 30.0 seconds..
2018-05-24 21:47:51 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://radius.inetm.pl/notification took longer than 30.0 seconds..
2018-05-24 21:47:55 [scrapy.core.scraper] ERROR: Error downloading <GET http://204.11.1.34:9999/download.2345.com/jifen_browser/2345_k1875408_browser.exe>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://204.11.1.34:9999/download.2345.com/jifen_browser/2345_k1875408_browser.exe took longer than 30.0 seconds..
2018-05-24 21:48:18 [scrapy.extensions.logstats] INFO: Crawled 3315 pages (at 30 pages/min), scraped 4193 items (at 42 items/min)
2018-05-24 21:48:20 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_12498474009&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129686056>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-05-24 21:48:41 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_10122431421&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129686056>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:48:50 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_10403248743&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129686025>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-05-24 21:48:55 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_11140264947&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129686056>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:48:56 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://radius.inetm.pl/notification took longer than 30.0 seconds..
2018-05-24 21:49:04 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_11382867638&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129686025>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:49:08 [scrapy.core.scraper] ERROR: Error downloading <GET http://download.2345.com/jifen_browser/2345_k1875408_browser.exe>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:49:16 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_10620164735&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129683822>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2018-05-24 21:49:18 [scrapy.extensions.logstats] INFO: Crawled 3348 pages (at 33 pages/min), scraped 4367 items (at 174 items/min)
2018-05-24 21:49:19 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_21680343692&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129686025>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:49:23 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_10157894595&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129686025>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:49:46 [scrapy.core.scraper] ERROR: Error downloading <GET http://download.2345.com/jifen_browser/2345_k1875408_browser.exe>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:49:47 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_25731696183&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129686025>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:50:03 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_5373174&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129685978>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2018-05-24 21:50:04 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_1560525653&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129685978>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:50:08 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_12580818483&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129685994>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:50:18 [scrapy.extensions.logstats] INFO: Crawled 3398 pages (at 50 pages/min), scraped 4689 items (at 322 items/min)
2018-05-24 21:50:24 [scrapy.core.scraper] ERROR: Error downloading <GET http://sclub.jd.com/comment/productPageComments.action?productId=11596888721&score=0&sortType=5&page=6&pageSize=10>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:50:48 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:50:53 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:50:57 [scrapy.core.scraper] ERROR: Error downloading <GET http://download.2345.com/jifen_browser/2345_k1875408_browser.exe>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2018-05-24 21:50:57 [scrapy.core.scraper] ERROR: Error downloading <GET http://sclub.jd.com/comment/productPageComments.action?productId=10236630040&score=0&sortType=5&page=2&pageSize=10>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:50:58 [scrapy.core.scraper] ERROR: Error downloading <GET http://sclub.jd.com/comment/productPageComments.action?productId=10581284915&score=0&sortType=5&page=10&pageSize=10>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2018-05-24 21:51:18 [scrapy.extensions.logstats] INFO: Crawled 3436 pages (at 38 pages/min), scraped 4757 items (at 68 items/min)
2018-05-24 21:51:51 [scrapy.core.scraper] ERROR: Error downloading <GET http://download.2345.com/jifen_browser/2345_k1875408_browser.exe>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:51:55 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_27341420994&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129685291>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:52:08 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_13732827829&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129685291>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:52:12 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_17865470036&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129685275>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:52:15 [scrapy.core.scraper] ERROR: Error downloading <GET http://download.2345.com/jifen_browser/2345_k1875408_browser.exe>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2018-05-24 21:52:16 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_26691737514&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129685259>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:52:18 [scrapy.extensions.logstats] INFO: Crawled 3490 pages (at 54 pages/min), scraped 4973 items (at 216 items/min)
2018-05-24 21:52:26 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_12580818486&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129685259>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:52:44 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_13732827818&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129685291>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:52:50 [scrapy.core.scraper] ERROR: Error downloading <GET http://sclub.jd.com/comment/productPageComments.action?productId=1649869880&score=0&sortType=5&page=44&pageSize=10>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2018-05-24 21:53:18 [scrapy.extensions.logstats] INFO: Crawled 3508 pages (at 18 pages/min), scraped 5143 items (at 170 items/min)
2018-05-24 21:53:48 [scrapy.core.scraper] ERROR: Error downloading <GET http://sclub.jd.com/comment/productPageComments.action?productId=1649869880&score=0&sortType=5&page=22&pageSize=10>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:53:54 [scrapy.core.scraper] ERROR: Error downloading <GET http://sclub.jd.com/comment/productPageComments.action?productId=1649869880&score=0&sortType=5&page=17&pageSize=10>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:54:17 [scrapy.core.scraper] ERROR: Error downloading <GET http://sclub.jd.com/comment/productPageComments.action?productId=1649869880&score=0&sortType=5&page=10&pageSize=10>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:54:18 [scrapy.extensions.logstats] INFO: Crawled 3542 pages (at 34 pages/min), scraped 5307 items (at 164 items/min)
2018-05-24 21:54:20 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_12716965328&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129685244>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2018-05-24 21:54:25 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:54:36 [scrapy.core.scraper] ERROR: Error downloading <GET http://204.11.1.34:9999/download.2345.com/jifen_browser/2345_k1875408_browser.exe>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://204.11.1.34:9999/download.2345.com/jifen_browser/2345_k1875408_browser.exe took longer than 30.0 seconds..
2018-05-24 21:54:37 [scrapy.core.scraper] ERROR: Error downloading <GET http://sclub.jd.com/comment/productPageComments.action?productId=11727843931&score=0&sortType=5&page=8&pageSize=10>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2018-05-24 21:54:38 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_10581284955&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129685244>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:55:03 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:55:15 [scrapy.core.scraper] ERROR: Error downloading <GET http://204.11.1.34:9999/183.91.33.92/download.2345.com/jifen_browser/2345_k1875408_browser.exe>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://204.11.1.34:9999/183.91.33.92/download.2345.com/jifen_browser/2345_k1875408_browser.exe took longer than 30.0 seconds..
2018-05-24 21:55:18 [scrapy.extensions.logstats] INFO: Crawled 3587 pages (at 45 pages/min), scraped 5481 items (at 174 items/min)
2018-05-24 21:55:18 [scrapy.core.scraper] ERROR: Error downloading <GET http://download.2345.com/jifen_browser/2345_k1875408_browser.exe>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2018-05-24 21:55:26 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://radius.inetm.pl/notification took longer than 30.0 seconds..
2018-05-24 21:55:30 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://radius.inetm.pl/notification took longer than 30.0 seconds..
2018-05-24 21:55:33 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_1078399212&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129684541>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:55:40 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_26756066112&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129685212>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:55:51 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_12822373259&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129685197>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:56:06 [scrapy.core.scraper] ERROR: Error downloading <GET http://club.jd.com/comment/productCommentSummaries.action?referenceIds=12733390869&_=1527170139059>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2018-05-24 21:56:07 [scrapy.core.scraper] ERROR: Error downloading <GET http://sclub.jd.com/comment/productPageComments.action?productId=27935403494&score=0&sortType=5&page=0&pageSize=10>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:56:10 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://radius.inetm.pl/notification took longer than 30.0 seconds..
2018-05-24 21:56:18 [scrapy.extensions.logstats] INFO: Crawled 3628 pages (at 41 pages/min), scraped 5605 items (at 124 items/min)
2018-05-24 21:56:21 [scrapy.core.scraper] ERROR: Error downloading <GET http://sclub.jd.com/comment/productPageComments.action?productId=12967756128&score=0&sortType=5&page=0&pageSize=10>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:56:22 [scrapy.core.scraper] ERROR: Error downloading <GET http://download.2345.com/jifen_browser/2345_k1875408_browser.exe>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:56:26 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_12711234565&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129685181>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2018-05-24 21:56:31 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_12822373226&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129685197>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:56:41 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_26756064561&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129685166>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:56:47 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_26756064590&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129685150>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:57:01 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_12822373260&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129685134>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_12822373260&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129685134 took longer than 30.0 seconds..
2018-05-24 21:57:14 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_26756064592&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129685150>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2018-05-24 21:57:14 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_1422153219&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129683775>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:57:18 [scrapy.extensions.logstats] INFO: Crawled 3671 pages (at 43 pages/min), scraped 5799 items (at 194 items/min)
2018-05-24 21:57:22 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_12967756122&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129685166>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:57:42 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:57:49 [scrapy.core.scraper] ERROR: Error downloading <GET http://sclub.jd.com/comment/productPageComments.action?productId=26756066119&score=0&sortType=5&page=2&pageSize=10>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:57:51 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_11957420524&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129684650>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:57:56 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_10236628494&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129683900>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:58:06 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:58:18 [scrapy.extensions.logstats] INFO: Crawled 3707 pages (at 36 pages/min), scraped 5853 items (at 54 items/min)
2018-05-24 21:58:37 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_11467022484&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129684634>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:58:50 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_1106491908&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129684634>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:58:52 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_21471728062&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129684634>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:58:58 [scrapy.core.scraper] ERROR: Error downloading <GET http://download.2345.com/jifen_browser/2345_k1875408_browser.exe>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:59:02 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_11467022497&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129684634>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:59:03 [scrapy.core.scraper] ERROR: Error downloading <GET http://download.2345.com/jifen_browser/2345_k1875408_browser.exe>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:59:12 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_11467022486&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129684634>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:59:18 [scrapy.extensions.logstats] INFO: Crawled 3729 pages (at 22 pages/min), scraped 5903 items (at 50 items/min)
2018-05-24 21:59:26 [scrapy.core.scraper] ERROR: Error downloading <GET http://radius.inetm.pl/notification>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:59:29 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_11467351464&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129684603>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 21:59:45 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_25729668254&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129684634>
Traceback (most recent call last):
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\python\failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\allen\Envs\scrapy_py3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_25729668254&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129684634 took longer than 30.0 seconds..
2018-05-24 22:00:05 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_1106491901&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129684603>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 22:00:06 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_4656531&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129684619>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2018-05-24 22:00:09 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_10296069018&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129684587>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2018-05-24 22:00:18 [scrapy.extensions.logstats] INFO: Crawled 3737 pages (at 8 pages/min), scraped 5903 items (at 0 items/min)
2018-05-24 22:00:21 [scrapy.core.scraper] ERROR: Error downloading <GET http://p.3.cn/prices/mgets?type=1&area=1_72_4137_0&skuIds=J_11467022482&pdpin=&pin=null&pdbp=0&pdtk=&pdpin=&source=list_pc_front&_=1527129684603>: TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
